{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c19c1932",
   "metadata": {
    "id": "c19c1932"
   },
   "outputs": [],
   "source": [
    "# Classification model using TF 2.0 to model the usefulness of ultasonic_flow\n",
    "# Ref: https://stackabuse.com/tensorflow-2-0-solving-classification-and-regression-problems/\n",
    "# Data files of ultasonic_flow: https://archive.ics.uci.edu/dataset/19/ultasonic_flow+evaluation\n",
    "# Ref2 Book: Gianultasonic_flowlo Zaccone, Getting Started with TensorFlow-Packt Publishing (2016)\n",
    "# Chapter 3 on Classifiers ]\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Ref for matplotlib: https://www.tutorialspoint.com/matplotlib/index.htm\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, Dropout\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1d6ccc",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "iGJN3yZmevQP",
   "metadata": {
    "id": "iGJN3yZmevQP"
   },
   "outputs": [],
   "source": [
    "def read_data(file_name):\n",
    "    with open(file_name, 'r') as file:\n",
    "        string = file.read()\n",
    "        data = []\n",
    "        rows = string.split('\\n')\n",
    "        for i in rows:\n",
    "            data.append(i.split('\\t'))\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "z5fd68hipfI-",
   "metadata": {
    "id": "z5fd68hipfI-"
   },
   "outputs": [],
   "source": [
    "data1 = read_data(\"./Meter D\")\n",
    "# data2 = read_data(\"./Meter C\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beIRK_RGkFyH",
   "metadata": {
    "id": "beIRK_RGkFyH"
   },
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "kP45p9lg-QLD",
   "metadata": {
    "id": "kP45p9lg-QLD"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4XkKmX0IkvKg",
   "metadata": {
    "id": "4XkKmX0IkvKg"
   },
   "outputs": [],
   "source": [
    "cols = ['Profile factor', 'Symmetry', 'Crossflow',\n",
    "        'Flow velocity 1', 'Flow velocity 2', 'Flow velocity 3', 'Flow velocity 4',\n",
    "        'Speed sound 1', 'Speed sound 2', 'Speed sound 3', 'Speed sound 4',\n",
    "        'Signal strength 1_1', 'Signal strength 1_2' , 'Signal strength 2_1' , 'Signal strength 2_2' , 'Signal strength 3_1' , 'Signal strength 3_2', 'Signal strength 4_1' , 'Signal strength 4_2',\n",
    "        'Signal quality 1_1', 'Signal quality 1_2' , 'Signal quality 2_1' , 'Signal quality 2_2' , 'Signal quality 3_1' , 'Signal quality 3_2', 'Signal quality 4_1' , 'Signal quality 4_2',\n",
    "        'Gain 1_1', 'Gain 1_2', 'Gain 2_1', 'Gain 2_2', 'Gain 3_1', 'Gain 3_2', 'Gain 4_1', 'Gain 4_2',\n",
    "        'Transit time 1_1', 'Transit time 1_2', 'Transit time 2_1', 'Transit time 2_2', 'Transit time 3_1', 'Transit time 3_2', 'Transit time 4_1', 'Transit time 4_2',\n",
    "        'class']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930daab8",
   "metadata": {},
   "source": [
    "Convert to dataframe and convert column `class` in a one-hot encoded form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "vX3W6J6YrDOB",
   "metadata": {
    "id": "vX3W6J6YrDOB"
   },
   "outputs": [],
   "source": [
    "# Import the CSV file to Panda's DataFrame format.\n",
    "# df1 = pd.DataFrame(data1, columns=cols)\n",
    "# df2 = pd.DataFrame(data2, columns=cols)\n",
    "# df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "df = pd.DataFrame(data1, columns=cols)\n",
    "df.dropna(inplace = True)\n",
    "df = pd.get_dummies(df, columns=['class'], prefix='class')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cb24d5",
   "metadata": {},
   "source": [
    "Drop columns which don't contribute that much to the output prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "oU1rvSUO-REP",
   "metadata": {
    "id": "oU1rvSUO-REP"
   },
   "outputs": [],
   "source": [
    "# Drop certain columns\n",
    "drop_cols = ['Transit time 1_1', 'Transit time 1_2', 'Transit time 2_1', 'Transit time 2_2', 'Transit time 3_1', 'Transit time 3_2', 'Transit time 4_1', 'Transit time 4_2',\n",
    "        'Signal strength 1_1', 'Signal strength 1_2' , 'Signal strength 2_1' , 'Signal strength 2_2' , 'Signal strength 3_1' , 'Signal strength 3_2', 'Signal strength 4_1' , 'Signal strength 4_2'\n",
    "             ]\n",
    "df.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "# Separate features and target columns\n",
    "class_columns = [col for col in df.columns if col.startswith('class')]\n",
    "feature_columns = [col for col in df.columns if (col not in class_columns) and col not in drop_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d81e6f",
   "metadata": {},
   "source": [
    "Normalize the continuos columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2kgN-Xv1Azxl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290
    },
    "id": "2kgN-Xv1Azxl",
    "outputId": "61b47898-da0a-4a18-a600-5c2e6f5d7b48"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Profile factor</th>\n",
       "      <th>Symmetry</th>\n",
       "      <th>Crossflow</th>\n",
       "      <th>Flow velocity 1</th>\n",
       "      <th>Flow velocity 2</th>\n",
       "      <th>Flow velocity 3</th>\n",
       "      <th>Flow velocity 4</th>\n",
       "      <th>Speed sound 1</th>\n",
       "      <th>Speed sound 2</th>\n",
       "      <th>Speed sound 3</th>\n",
       "      <th>...</th>\n",
       "      <th>Gain 2_1</th>\n",
       "      <th>Gain 2_2</th>\n",
       "      <th>Gain 3_1</th>\n",
       "      <th>Gain 3_2</th>\n",
       "      <th>Gain 4_1</th>\n",
       "      <th>Gain 4_2</th>\n",
       "      <th>class_1</th>\n",
       "      <th>class_2</th>\n",
       "      <th>class_3</th>\n",
       "      <th>class_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.087538</td>\n",
       "      <td>-0.062980</td>\n",
       "      <td>-0.254455</td>\n",
       "      <td>-0.874991</td>\n",
       "      <td>-0.964069</td>\n",
       "      <td>-0.973949</td>\n",
       "      <td>-0.180545</td>\n",
       "      <td>0.287334</td>\n",
       "      <td>0.336721</td>\n",
       "      <td>0.365831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.037560</td>\n",
       "      <td>-0.402813</td>\n",
       "      <td>-0.042251</td>\n",
       "      <td>-0.508207</td>\n",
       "      <td>-0.608418</td>\n",
       "      <td>-0.610085</td>\n",
       "      <td>0.002792</td>\n",
       "      <td>0.399826</td>\n",
       "      <td>0.460508</td>\n",
       "      <td>0.486537</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.115942</td>\n",
       "      <td>-0.110345</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.117088</td>\n",
       "      <td>0.004812</td>\n",
       "      <td>0.024442</td>\n",
       "      <td>-0.494766</td>\n",
       "      <td>-0.607258</td>\n",
       "      <td>-0.617229</td>\n",
       "      <td>0.003746</td>\n",
       "      <td>0.449531</td>\n",
       "      <td>0.510393</td>\n",
       "      <td>0.537604</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.115942</td>\n",
       "      <td>-0.110345</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.025841</td>\n",
       "      <td>0.356336</td>\n",
       "      <td>-0.246027</td>\n",
       "      <td>-0.508207</td>\n",
       "      <td>-0.603955</td>\n",
       "      <td>-0.626604</td>\n",
       "      <td>-0.004653</td>\n",
       "      <td>0.491389</td>\n",
       "      <td>0.551039</td>\n",
       "      <td>0.580316</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.115942</td>\n",
       "      <td>-0.110345</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.001474</td>\n",
       "      <td>0.185034</td>\n",
       "      <td>0.326474</td>\n",
       "      <td>-0.508401</td>\n",
       "      <td>-0.611096</td>\n",
       "      <td>-0.619908</td>\n",
       "      <td>-0.007945</td>\n",
       "      <td>0.536734</td>\n",
       "      <td>0.600924</td>\n",
       "      <td>0.629526</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.115942</td>\n",
       "      <td>-0.110345</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Profile factor  Symmetry  Crossflow  Flow velocity 1  Flow velocity 2  \\\n",
       "0        0.087538 -0.062980  -0.254455        -0.874991        -0.964069   \n",
       "1       -0.037560 -0.402813  -0.042251        -0.508207        -0.608418   \n",
       "2       -0.117088  0.004812   0.024442        -0.494766        -0.607258   \n",
       "3       -0.025841  0.356336  -0.246027        -0.508207        -0.603955   \n",
       "4       -0.001474  0.185034   0.326474        -0.508401        -0.611096   \n",
       "\n",
       "   Flow velocity 3  Flow velocity 4  Speed sound 1  Speed sound 2  \\\n",
       "0        -0.973949        -0.180545       0.287334       0.336721   \n",
       "1        -0.610085         0.002792       0.399826       0.460508   \n",
       "2        -0.617229         0.003746       0.449531       0.510393   \n",
       "3        -0.626604        -0.004653       0.491389       0.551039   \n",
       "4        -0.619908        -0.007945       0.536734       0.600924   \n",
       "\n",
       "   Speed sound 3  ...  Gain 2_1  Gain 2_2  Gain 3_1  Gain 3_2  Gain 4_1  \\\n",
       "0       0.365831  ...  0.000000  0.000000       0.0       0.0       0.0   \n",
       "1       0.486537  ... -0.115942 -0.110345       0.0       0.0       0.0   \n",
       "2       0.537604  ... -0.115942 -0.110345       0.0       0.0       0.0   \n",
       "3       0.580316  ... -0.115942 -0.110345       0.0       0.0       0.0   \n",
       "4       0.629526  ... -0.115942 -0.110345       0.0       0.0       0.0   \n",
       "\n",
       "   Gain 4_2  class_1  class_2  class_3  class_4  \n",
       "0       0.0        1        0        0        0  \n",
       "1       0.0        1        0        0        0  \n",
       "2       0.0        1        0        0        0  \n",
       "3       0.0        1        0        0        0  \n",
       "4       0.0        1        0        0        0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize the feature columns\n",
    "scaler = RobustScaler()\n",
    "df[feature_columns] = scaler.fit_transform(df[feature_columns])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6eddb64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Profile factor</th>\n",
       "      <th>Symmetry</th>\n",
       "      <th>Crossflow</th>\n",
       "      <th>Flow velocity 1</th>\n",
       "      <th>Flow velocity 2</th>\n",
       "      <th>Flow velocity 3</th>\n",
       "      <th>Flow velocity 4</th>\n",
       "      <th>Speed sound 1</th>\n",
       "      <th>Speed sound 2</th>\n",
       "      <th>Speed sound 3</th>\n",
       "      <th>...</th>\n",
       "      <th>Gain 2_1</th>\n",
       "      <th>Gain 2_2</th>\n",
       "      <th>Gain 3_1</th>\n",
       "      <th>Gain 3_2</th>\n",
       "      <th>Gain 4_1</th>\n",
       "      <th>Gain 4_2</th>\n",
       "      <th>class_1</th>\n",
       "      <th>class_2</th>\n",
       "      <th>class_3</th>\n",
       "      <th>class_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.800000e+02</td>\n",
       "      <td>1.800000e+02</td>\n",
       "      <td>1.800000e+02</td>\n",
       "      <td>1.800000e+02</td>\n",
       "      <td>1.800000e+02</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.597108e+00</td>\n",
       "      <td>7.914769e+00</td>\n",
       "      <td>1.247451e+01</td>\n",
       "      <td>-1.336943e-01</td>\n",
       "      <td>-2.292785e-01</td>\n",
       "      <td>-0.224850</td>\n",
       "      <td>-0.022702</td>\n",
       "      <td>0.752479</td>\n",
       "      <td>-0.125784</td>\n",
       "      <td>0.348329</td>\n",
       "      <td>...</td>\n",
       "      <td>3.755717</td>\n",
       "      <td>3.574704</td>\n",
       "      <td>18.673310</td>\n",
       "      <td>18.486243</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.294083</td>\n",
       "      <td>0.283333</td>\n",
       "      <td>0.127778</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>0.283333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.390717e+00</td>\n",
       "      <td>1.570499e+02</td>\n",
       "      <td>4.440244e+01</td>\n",
       "      <td>7.602828e-01</td>\n",
       "      <td>7.575564e-01</td>\n",
       "      <td>0.716565</td>\n",
       "      <td>0.505894</td>\n",
       "      <td>3.399379</td>\n",
       "      <td>3.628441</td>\n",
       "      <td>2.494436</td>\n",
       "      <td>...</td>\n",
       "      <td>7.699420</td>\n",
       "      <td>7.328046</td>\n",
       "      <td>38.223136</td>\n",
       "      <td>37.832646</td>\n",
       "      <td>0.452800</td>\n",
       "      <td>0.452821</td>\n",
       "      <td>0.451874</td>\n",
       "      <td>0.334773</td>\n",
       "      <td>0.461927</td>\n",
       "      <td>0.451874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.916546e+01</td>\n",
       "      <td>-1.909999e+03</td>\n",
       "      <td>-2.151844e+02</td>\n",
       "      <td>-2.602732e+00</td>\n",
       "      <td>-2.483798e+00</td>\n",
       "      <td>-1.864366</td>\n",
       "      <td>-1.104051</td>\n",
       "      <td>-13.566601</td>\n",
       "      <td>-23.625404</td>\n",
       "      <td>-8.739090</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.173913</td>\n",
       "      <td>-0.165517</td>\n",
       "      <td>-1.237113</td>\n",
       "      <td>-1.224490</td>\n",
       "      <td>-0.006682</td>\n",
       "      <td>-0.006682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-7.055786e-02</td>\n",
       "      <td>-3.052505e-01</td>\n",
       "      <td>-4.042829e-01</td>\n",
       "      <td>-5.157500e-01</td>\n",
       "      <td>-6.108284e-01</td>\n",
       "      <td>-0.626537</td>\n",
       "      <td>-0.519660</td>\n",
       "      <td>-0.443863</td>\n",
       "      <td>-0.445035</td>\n",
       "      <td>-0.418059</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.115942</td>\n",
       "      <td>-0.110345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-9.074772e-16</td>\n",
       "      <td>-5.467415e-15</td>\n",
       "      <td>-3.648123e-15</td>\n",
       "      <td>1.526557e-16</td>\n",
       "      <td>-1.426810e-16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.294421e-01</td>\n",
       "      <td>6.947495e-01</td>\n",
       "      <td>5.957171e-01</td>\n",
       "      <td>4.842500e-01</td>\n",
       "      <td>3.891716e-01</td>\n",
       "      <td>0.373463</td>\n",
       "      <td>0.480340</td>\n",
       "      <td>0.556137</td>\n",
       "      <td>0.554965</td>\n",
       "      <td>0.581941</td>\n",
       "      <td>...</td>\n",
       "      <td>0.884058</td>\n",
       "      <td>0.889655</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.937576e+01</td>\n",
       "      <td>5.015842e+02</td>\n",
       "      <td>1.521693e+02</td>\n",
       "      <td>1.107651e+00</td>\n",
       "      <td>9.843332e-01</td>\n",
       "      <td>0.968413</td>\n",
       "      <td>0.780564</td>\n",
       "      <td>21.184652</td>\n",
       "      <td>12.904850</td>\n",
       "      <td>9.316620</td>\n",
       "      <td>...</td>\n",
       "      <td>25.507246</td>\n",
       "      <td>24.275862</td>\n",
       "      <td>107.381443</td>\n",
       "      <td>106.285714</td>\n",
       "      <td>1.020045</td>\n",
       "      <td>1.020045</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Profile factor      Symmetry     Crossflow  Flow velocity 1  \\\n",
       "count    1.800000e+02  1.800000e+02  1.800000e+02     1.800000e+02   \n",
       "mean     2.597108e+00  7.914769e+00  1.247451e+01    -1.336943e-01   \n",
       "std      7.390717e+00  1.570499e+02  4.440244e+01     7.602828e-01   \n",
       "min     -2.916546e+01 -1.909999e+03 -2.151844e+02    -2.602732e+00   \n",
       "25%     -7.055786e-02 -3.052505e-01 -4.042829e-01    -5.157500e-01   \n",
       "50%     -9.074772e-16 -5.467415e-15 -3.648123e-15     1.526557e-16   \n",
       "75%      9.294421e-01  6.947495e-01  5.957171e-01     4.842500e-01   \n",
       "max      3.937576e+01  5.015842e+02  1.521693e+02     1.107651e+00   \n",
       "\n",
       "       Flow velocity 2  Flow velocity 3  Flow velocity 4  Speed sound 1  \\\n",
       "count     1.800000e+02       180.000000       180.000000     180.000000   \n",
       "mean     -2.292785e-01        -0.224850        -0.022702       0.752479   \n",
       "std       7.575564e-01         0.716565         0.505894       3.399379   \n",
       "min      -2.483798e+00        -1.864366        -1.104051     -13.566601   \n",
       "25%      -6.108284e-01        -0.626537        -0.519660      -0.443863   \n",
       "50%      -1.426810e-16         0.000000         0.000000       0.000000   \n",
       "75%       3.891716e-01         0.373463         0.480340       0.556137   \n",
       "max       9.843332e-01         0.968413         0.780564      21.184652   \n",
       "\n",
       "       Speed sound 2  Speed sound 3  ...    Gain 2_1    Gain 2_2    Gain 3_1  \\\n",
       "count     180.000000     180.000000  ...  180.000000  180.000000  180.000000   \n",
       "mean       -0.125784       0.348329  ...    3.755717    3.574704   18.673310   \n",
       "std         3.628441       2.494436  ...    7.699420    7.328046   38.223136   \n",
       "min       -23.625404      -8.739090  ...   -0.173913   -0.165517   -1.237113   \n",
       "25%        -0.445035      -0.418059  ...   -0.115942   -0.110345    0.000000   \n",
       "50%         0.000000       0.000000  ...    0.000000    0.000000    0.000000   \n",
       "75%         0.554965       0.581941  ...    0.884058    0.889655    1.000000   \n",
       "max        12.904850       9.316620  ...   25.507246   24.275862  107.381443   \n",
       "\n",
       "         Gain 3_2    Gain 4_1    Gain 4_2     class_1     class_2     class_3  \\\n",
       "count  180.000000  180.000000  180.000000  180.000000  180.000000  180.000000   \n",
       "mean    18.486243    0.294118    0.294083    0.283333    0.127778    0.305556   \n",
       "std     37.832646    0.452800    0.452821    0.451874    0.334773    0.461927   \n",
       "min     -1.224490   -0.006682   -0.006682    0.000000    0.000000    0.000000   \n",
       "25%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "50%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "75%      1.000000    1.000000    1.000000    1.000000    0.000000    1.000000   \n",
       "max    106.285714    1.020045    1.020045    1.000000    1.000000    1.000000   \n",
       "\n",
       "          class_4  \n",
       "count  180.000000  \n",
       "mean     0.283333  \n",
       "std      0.451874  \n",
       "min      0.000000  \n",
       "25%      0.000000  \n",
       "50%      0.000000  \n",
       "75%      1.000000  \n",
       "max      1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b896e7",
   "metadata": {},
   "source": [
    "Split the dataset to features and target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5EmPDsbMrGH2",
   "metadata": {
    "id": "5EmPDsbMrGH2"
   },
   "outputs": [],
   "source": [
    "features = df.drop(columns=class_columns)\n",
    "target = df[class_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2sIx6b44kOtC",
   "metadata": {
    "id": "2sIx6b44kOtC"
   },
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d27ac628",
   "metadata": {
    "id": "d27ac628"
   },
   "outputs": [],
   "source": [
    "X = features\n",
    "y = target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa8db1d",
   "metadata": {},
   "source": [
    "Making sure of the number of cols ans rows in dataset is accounted for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "37b3c912",
   "metadata": {
    "id": "37b3c912",
    "outputId": "6a93c455-d95a-46c1-cf6f-bbf030c3a85e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180, 27)\n",
      "(180, 4)\n",
      "Shape of X_train and X_test: (144, 27) (36, 27)\n",
      "Shape of y_train and y_test: (144, 4) (36, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# The total data in the dataset is 1728, 20% of them is: 346 (no. of test data)\n",
    "# Size of train data: 1728 - 346 = 1382\n",
    "# random_state is a seed. It can be any value. If we keep it same on every\n",
    "# run, then there will be repeatability in the results.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,\n",
    "                                                    random_state=42)\n",
    "print('Shape of X_train and X_test:', X_train.shape, X_test.shape)\n",
    "print('Shape of y_train and y_test:', y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6df5b74",
   "metadata": {},
   "source": [
    "Checking datatypes and converting dataset values to `uint8` or `float8` for model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "EwSZ9wT67jeU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EwSZ9wT67jeU",
    "outputId": "80872713-2b22-4d3c-c8ad-efe3d262610b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64 (144, 27)\n",
      "uint8 (144, 4)\n"
     ]
    }
   ],
   "source": [
    "X_train_ = X_train.to_numpy()\n",
    "y_train_ = y_train.to_numpy()\n",
    "print(X_train_.dtype, X_train_.shape)\n",
    "print(y_train_.dtype, y_train_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "GzSce-1z8ZcR",
   "metadata": {
    "id": "GzSce-1z8ZcR"
   },
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "y_test = y_test.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4cc107",
   "metadata": {},
   "source": [
    "Model compialation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2d745df5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2d745df5",
    "outputId": "24903b52-813c-412d-c86d-4cb27e7f9c1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 4\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define the number of nodes in each layer of the network\n",
    "DENSE1_SIZE = 24\n",
    "DENSE2_SIZE = 18\n",
    "DENSE3_SIZE = 12\n",
    "# DENSE4_SIZE = 8\n",
    "DROPOUT_RATE = 0.3\n",
    "\n",
    "input_shape = X.shape[1]\n",
    "output_shape = y.shape[1]\n",
    "\n",
    "# Start the model with an Input layer\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape=(input_shape,)))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(DENSE1_SIZE, activation='relu'))\n",
    "# model.add(tf.keras.layers.Dropout(DROPOUT_RATE))\n",
    "model.add(tf.keras.layers.Dense(DENSE2_SIZE, activation='relu'))\n",
    "# model.add(tf.keras.layers.Dropout(DROPOUT_RATE))\n",
    "model.add(tf.keras.layers.Dense(DENSE3_SIZE, activation='relu'))\n",
    "# model.add(tf.keras.layers.Dense(DENSE4_SIZE, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(output_shape, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "print(input_shape, output_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "21f85e92",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315
    },
    "id": "21f85e92",
    "outputId": "c5c44ab0-2488-400f-e863-8c5e34b3beb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 24)                672       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 18)                450       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 12)                228       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4)                 52        \n",
      "=================================================================\n",
      "Total params: 1,402\n",
      "Trainable params: 1,402\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca902d6",
   "metadata": {},
   "source": [
    "Model Tranining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8pZ_81-l8u6U",
   "metadata": {
    "id": "8pZ_81-l8u6U"
   },
   "outputs": [],
   "source": [
    "NUM_OF_EPOCHS = 400\n",
    "BATCH_SIZE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e77dde18",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e77dde18",
    "outputId": "9f20e59f-2690-4507-89e1-1b42331f785e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "58/58 [==============================] - 0s 851us/step - loss: 1.9433e-04 - acc: 1.0000 - val_loss: 2.9531 - val_acc: 0.8621\n",
      "Epoch 2/400\n",
      "58/58 [==============================] - 0s 498us/step - loss: 1.8684e-04 - acc: 1.0000 - val_loss: 2.9248 - val_acc: 0.8621\n",
      "Epoch 3/400\n",
      "58/58 [==============================] - 0s 423us/step - loss: 1.3101e-04 - acc: 1.0000 - val_loss: 2.9398 - val_acc: 0.8621\n",
      "Epoch 4/400\n",
      "58/58 [==============================] - 0s 421us/step - loss: 1.2279e-04 - acc: 1.0000 - val_loss: 2.9848 - val_acc: 0.8621\n",
      "Epoch 5/400\n",
      "58/58 [==============================] - 0s 412us/step - loss: 1.3240e-04 - acc: 1.0000 - val_loss: 2.9995 - val_acc: 0.8621\n",
      "Epoch 6/400\n",
      "58/58 [==============================] - 0s 426us/step - loss: 1.1944e-04 - acc: 1.0000 - val_loss: 2.9698 - val_acc: 0.8621\n",
      "Epoch 7/400\n",
      "58/58 [==============================] - 0s 417us/step - loss: 1.1598e-04 - acc: 1.0000 - val_loss: 2.9774 - val_acc: 0.8621\n",
      "Epoch 8/400\n",
      "58/58 [==============================] - 0s 409us/step - loss: 1.0783e-04 - acc: 1.0000 - val_loss: 3.0011 - val_acc: 0.8621\n",
      "Epoch 9/400\n",
      "58/58 [==============================] - 0s 411us/step - loss: 1.0010e-04 - acc: 1.0000 - val_loss: 2.9905 - val_acc: 0.8621\n",
      "Epoch 10/400\n",
      "58/58 [==============================] - 0s 408us/step - loss: 1.0630e-04 - acc: 1.0000 - val_loss: 3.0178 - val_acc: 0.8621\n",
      "Epoch 11/400\n",
      "58/58 [==============================] - 0s 412us/step - loss: 9.6661e-05 - acc: 1.0000 - val_loss: 3.0405 - val_acc: 0.8621\n",
      "Epoch 12/400\n",
      "58/58 [==============================] - 0s 413us/step - loss: 1.0117e-04 - acc: 1.0000 - val_loss: 3.0755 - val_acc: 0.8621\n",
      "Epoch 13/400\n",
      "58/58 [==============================] - 0s 411us/step - loss: 8.8376e-05 - acc: 1.0000 - val_loss: 3.0743 - val_acc: 0.8621\n",
      "Epoch 14/400\n",
      "58/58 [==============================] - 0s 399us/step - loss: 8.5318e-05 - acc: 1.0000 - val_loss: 3.0499 - val_acc: 0.8621\n",
      "Epoch 15/400\n",
      "58/58 [==============================] - 0s 406us/step - loss: 8.3107e-05 - acc: 1.0000 - val_loss: 3.0705 - val_acc: 0.8621\n",
      "Epoch 16/400\n",
      "58/58 [==============================] - 0s 423us/step - loss: 8.8298e-05 - acc: 1.0000 - val_loss: 3.1050 - val_acc: 0.8621\n",
      "Epoch 17/400\n",
      "58/58 [==============================] - 0s 415us/step - loss: 8.5971e-05 - acc: 1.0000 - val_loss: 3.0911 - val_acc: 0.8621\n",
      "Epoch 18/400\n",
      "58/58 [==============================] - 0s 412us/step - loss: 7.9653e-05 - acc: 1.0000 - val_loss: 3.0953 - val_acc: 0.8621\n",
      "Epoch 19/400\n",
      "58/58 [==============================] - 0s 412us/step - loss: 7.5989e-05 - acc: 1.0000 - val_loss: 3.1158 - val_acc: 0.8621\n",
      "Epoch 20/400\n",
      "58/58 [==============================] - 0s 405us/step - loss: 7.7614e-05 - acc: 1.0000 - val_loss: 3.1008 - val_acc: 0.8621\n",
      "Epoch 21/400\n",
      "58/58 [==============================] - 0s 413us/step - loss: 7.5366e-05 - acc: 1.0000 - val_loss: 3.1532 - val_acc: 0.8621\n",
      "Epoch 22/400\n",
      "58/58 [==============================] - 0s 421us/step - loss: 7.2246e-05 - acc: 1.0000 - val_loss: 3.0832 - val_acc: 0.8621\n",
      "Epoch 23/400\n",
      "58/58 [==============================] - 0s 411us/step - loss: 8.6047e-05 - acc: 1.0000 - val_loss: 3.1275 - val_acc: 0.8621\n",
      "Epoch 24/400\n",
      "58/58 [==============================] - 0s 417us/step - loss: 6.7396e-05 - acc: 1.0000 - val_loss: 3.1227 - val_acc: 0.8621\n",
      "Epoch 25/400\n",
      "58/58 [==============================] - 0s 416us/step - loss: 6.4953e-05 - acc: 1.0000 - val_loss: 3.1653 - val_acc: 0.8621\n",
      "Epoch 26/400\n",
      "58/58 [==============================] - 0s 409us/step - loss: 6.3240e-05 - acc: 1.0000 - val_loss: 3.1537 - val_acc: 0.8621\n",
      "Epoch 27/400\n",
      "58/58 [==============================] - 0s 417us/step - loss: 5.9228e-05 - acc: 1.0000 - val_loss: 3.1604 - val_acc: 0.8621\n",
      "Epoch 28/400\n",
      "58/58 [==============================] - 0s 406us/step - loss: 6.1128e-05 - acc: 1.0000 - val_loss: 3.1866 - val_acc: 0.8621\n",
      "Epoch 29/400\n",
      "58/58 [==============================] - 0s 414us/step - loss: 5.9077e-05 - acc: 1.0000 - val_loss: 3.1647 - val_acc: 0.8621\n",
      "Epoch 30/400\n",
      "58/58 [==============================] - 0s 411us/step - loss: 6.4280e-05 - acc: 1.0000 - val_loss: 3.2126 - val_acc: 0.8621\n",
      "Epoch 31/400\n",
      "58/58 [==============================] - 0s 413us/step - loss: 6.3491e-05 - acc: 1.0000 - val_loss: 3.1967 - val_acc: 0.8621\n",
      "Epoch 32/400\n",
      "58/58 [==============================] - 0s 413us/step - loss: 5.1999e-05 - acc: 1.0000 - val_loss: 3.2129 - val_acc: 0.8621\n",
      "Epoch 33/400\n",
      "58/58 [==============================] - 0s 406us/step - loss: 5.1756e-05 - acc: 1.0000 - val_loss: 3.2164 - val_acc: 0.8621\n",
      "Epoch 34/400\n",
      "58/58 [==============================] - 0s 412us/step - loss: 4.9007e-05 - acc: 1.0000 - val_loss: 3.2299 - val_acc: 0.8621\n",
      "Epoch 35/400\n",
      "58/58 [==============================] - 0s 410us/step - loss: 4.8805e-05 - acc: 1.0000 - val_loss: 3.2373 - val_acc: 0.8621\n",
      "Epoch 36/400\n",
      "58/58 [==============================] - 0s 408us/step - loss: 4.9618e-05 - acc: 1.0000 - val_loss: 3.2447 - val_acc: 0.8621\n",
      "Epoch 37/400\n",
      "58/58 [==============================] - 0s 412us/step - loss: 4.3904e-05 - acc: 1.0000 - val_loss: 3.2495 - val_acc: 0.8621\n",
      "Epoch 38/400\n",
      "58/58 [==============================] - 0s 397us/step - loss: 4.2839e-05 - acc: 1.0000 - val_loss: 3.2784 - val_acc: 0.8621\n",
      "Epoch 39/400\n",
      "58/58 [==============================] - 0s 401us/step - loss: 5.0867e-05 - acc: 1.0000 - val_loss: 3.2814 - val_acc: 0.8621\n",
      "Epoch 40/400\n",
      "58/58 [==============================] - 0s 406us/step - loss: 4.4413e-05 - acc: 1.0000 - val_loss: 3.2492 - val_acc: 0.8621\n",
      "Epoch 41/400\n",
      "58/58 [==============================] - 0s 405us/step - loss: 4.4298e-05 - acc: 1.0000 - val_loss: 3.3113 - val_acc: 0.8621\n",
      "Epoch 42/400\n",
      "58/58 [==============================] - 0s 402us/step - loss: 4.0616e-05 - acc: 1.0000 - val_loss: 3.3035 - val_acc: 0.8621\n",
      "Epoch 43/400\n",
      "58/58 [==============================] - 0s 399us/step - loss: 4.3155e-05 - acc: 1.0000 - val_loss: 3.2928 - val_acc: 0.8621\n",
      "Epoch 44/400\n",
      "58/58 [==============================] - 0s 399us/step - loss: 3.8092e-05 - acc: 1.0000 - val_loss: 3.3420 - val_acc: 0.8621\n",
      "Epoch 45/400\n",
      "58/58 [==============================] - 0s 396us/step - loss: 3.7969e-05 - acc: 1.0000 - val_loss: 3.3364 - val_acc: 0.8621\n",
      "Epoch 46/400\n",
      "58/58 [==============================] - 0s 407us/step - loss: 3.3915e-05 - acc: 1.0000 - val_loss: 3.3249 - val_acc: 0.8621\n",
      "Epoch 47/400\n",
      "58/58 [==============================] - 0s 411us/step - loss: 3.3291e-05 - acc: 1.0000 - val_loss: 3.3474 - val_acc: 0.8621\n",
      "Epoch 48/400\n",
      "58/58 [==============================] - 0s 400us/step - loss: 3.3508e-05 - acc: 1.0000 - val_loss: 3.3096 - val_acc: 0.8621\n",
      "Epoch 49/400\n",
      "58/58 [==============================] - 0s 411us/step - loss: 3.6875e-05 - acc: 1.0000 - val_loss: 3.3867 - val_acc: 0.8621\n",
      "Epoch 50/400\n",
      "58/58 [==============================] - 0s 415us/step - loss: 4.3855e-05 - acc: 1.0000 - val_loss: 3.2972 - val_acc: 0.8621\n",
      "Epoch 51/400\n",
      "58/58 [==============================] - 0s 414us/step - loss: 5.9947e-05 - acc: 1.0000 - val_loss: 3.3809 - val_acc: 0.8621\n",
      "Epoch 52/400\n",
      "58/58 [==============================] - 0s 410us/step - loss: 3.5116e-05 - acc: 1.0000 - val_loss: 3.4082 - val_acc: 0.8621\n",
      "Epoch 53/400\n",
      "58/58 [==============================] - 0s 414us/step - loss: 3.1695e-05 - acc: 1.0000 - val_loss: 3.4066 - val_acc: 0.8621\n",
      "Epoch 54/400\n",
      "58/58 [==============================] - 0s 417us/step - loss: 2.8244e-05 - acc: 1.0000 - val_loss: 3.4274 - val_acc: 0.8621\n",
      "Epoch 55/400\n",
      "58/58 [==============================] - 0s 411us/step - loss: 2.5918e-05 - acc: 1.0000 - val_loss: 3.3935 - val_acc: 0.8621\n",
      "Epoch 56/400\n",
      "58/58 [==============================] - 0s 412us/step - loss: 2.7754e-05 - acc: 1.0000 - val_loss: 3.4285 - val_acc: 0.8621\n",
      "Epoch 57/400\n",
      "58/58 [==============================] - 0s 415us/step - loss: 2.3675e-05 - acc: 1.0000 - val_loss: 3.4473 - val_acc: 0.8621\n",
      "Epoch 58/400\n",
      "58/58 [==============================] - 0s 416us/step - loss: 2.3974e-05 - acc: 1.0000 - val_loss: 3.4728 - val_acc: 0.8621\n",
      "Epoch 59/400\n",
      "58/58 [==============================] - 0s 417us/step - loss: 2.2150e-05 - acc: 1.0000 - val_loss: 3.4527 - val_acc: 0.8621\n",
      "Epoch 60/400\n",
      "58/58 [==============================] - 0s 406us/step - loss: 2.4759e-05 - acc: 1.0000 - val_loss: 3.4633 - val_acc: 0.8621\n",
      "Epoch 61/400\n",
      "58/58 [==============================] - 0s 414us/step - loss: 2.0745e-05 - acc: 1.0000 - val_loss: 3.4733 - val_acc: 0.8621\n",
      "Epoch 62/400\n",
      "58/58 [==============================] - 0s 406us/step - loss: 2.3914e-05 - acc: 1.0000 - val_loss: 3.5262 - val_acc: 0.8621\n",
      "Epoch 63/400\n",
      "58/58 [==============================] - 0s 413us/step - loss: 2.2194e-05 - acc: 1.0000 - val_loss: 3.4792 - val_acc: 0.8621\n",
      "Epoch 64/400\n",
      "58/58 [==============================] - 0s 409us/step - loss: 2.2648e-05 - acc: 1.0000 - val_loss: 3.5233 - val_acc: 0.8621\n",
      "Epoch 65/400\n",
      "58/58 [==============================] - 0s 409us/step - loss: 1.9829e-05 - acc: 1.0000 - val_loss: 3.5391 - val_acc: 0.8621\n",
      "Epoch 66/400\n",
      "58/58 [==============================] - 0s 414us/step - loss: 1.9490e-05 - acc: 1.0000 - val_loss: 3.5401 - val_acc: 0.8621\n",
      "Epoch 67/400\n",
      "58/58 [==============================] - 0s 654us/step - loss: 1.8516e-05 - acc: 1.0000 - val_loss: 3.5399 - val_acc: 0.8621\n",
      "Epoch 68/400\n",
      "58/58 [==============================] - 0s 413us/step - loss: 1.9493e-05 - acc: 1.0000 - val_loss: 3.5485 - val_acc: 0.8621\n",
      "Epoch 69/400\n",
      "58/58 [==============================] - 0s 556us/step - loss: 1.7811e-05 - acc: 1.0000 - val_loss: 3.5819 - val_acc: 0.8621\n",
      "Epoch 70/400\n",
      "58/58 [==============================] - 0s 410us/step - loss: 1.9091e-05 - acc: 1.0000 - val_loss: 3.5750 - val_acc: 0.8621\n",
      "Epoch 71/400\n",
      "58/58 [==============================] - 0s 409us/step - loss: 1.8127e-05 - acc: 1.0000 - val_loss: 3.5998 - val_acc: 0.8621\n",
      "Epoch 72/400\n",
      "58/58 [==============================] - 0s 414us/step - loss: 1.6288e-05 - acc: 1.0000 - val_loss: 3.5880 - val_acc: 0.8621\n",
      "Epoch 73/400\n",
      "58/58 [==============================] - 0s 412us/step - loss: 1.5189e-05 - acc: 1.0000 - val_loss: 3.6168 - val_acc: 0.8621\n",
      "Epoch 74/400\n",
      "58/58 [==============================] - 0s 406us/step - loss: 1.5253e-05 - acc: 1.0000 - val_loss: 3.6196 - val_acc: 0.8621\n",
      "Epoch 75/400\n",
      "58/58 [==============================] - 0s 406us/step - loss: 1.4844e-05 - acc: 1.0000 - val_loss: 3.6428 - val_acc: 0.8621\n",
      "Epoch 76/400\n",
      "58/58 [==============================] - 0s 408us/step - loss: 1.4839e-05 - acc: 1.0000 - val_loss: 3.6366 - val_acc: 0.8621\n",
      "Epoch 77/400\n",
      "58/58 [==============================] - 0s 401us/step - loss: 1.4417e-05 - acc: 1.0000 - val_loss: 3.6576 - val_acc: 0.8621\n",
      "Epoch 78/400\n",
      "58/58 [==============================] - 0s 411us/step - loss: 1.1916e-05 - acc: 1.0000 - val_loss: 3.6736 - val_acc: 0.8621\n",
      "Epoch 79/400\n",
      "58/58 [==============================] - 0s 414us/step - loss: 1.1193e-05 - acc: 1.0000 - val_loss: 3.6715 - val_acc: 0.8621\n",
      "Epoch 80/400\n",
      "58/58 [==============================] - 0s 408us/step - loss: 1.2473e-05 - acc: 1.0000 - val_loss: 3.6919 - val_acc: 0.8621\n",
      "Epoch 81/400\n",
      "58/58 [==============================] - 0s 413us/step - loss: 1.1918e-05 - acc: 1.0000 - val_loss: 3.6959 - val_acc: 0.8621\n",
      "Epoch 82/400\n",
      "58/58 [==============================] - 0s 415us/step - loss: 1.2595e-05 - acc: 1.0000 - val_loss: 3.6985 - val_acc: 0.8621\n",
      "Epoch 83/400\n",
      "58/58 [==============================] - 0s 405us/step - loss: 1.1300e-05 - acc: 1.0000 - val_loss: 3.7390 - val_acc: 0.8621\n",
      "Epoch 84/400\n",
      "58/58 [==============================] - 0s 408us/step - loss: 1.0555e-05 - acc: 1.0000 - val_loss: 3.7032 - val_acc: 0.8621\n",
      "Epoch 85/400\n",
      "58/58 [==============================] - 0s 408us/step - loss: 1.4342e-05 - acc: 1.0000 - val_loss: 3.7121 - val_acc: 0.8621\n",
      "Epoch 86/400\n",
      "58/58 [==============================] - 0s 414us/step - loss: 9.7753e-06 - acc: 1.0000 - val_loss: 3.7491 - val_acc: 0.8621\n",
      "Epoch 87/400\n",
      "58/58 [==============================] - 0s 407us/step - loss: 9.5806e-06 - acc: 1.0000 - val_loss: 3.7642 - val_acc: 0.8621\n",
      "Epoch 88/400\n",
      "58/58 [==============================] - 0s 405us/step - loss: 9.3535e-06 - acc: 1.0000 - val_loss: 3.7745 - val_acc: 0.8621\n",
      "Epoch 89/400\n",
      "58/58 [==============================] - 0s 412us/step - loss: 9.5100e-06 - acc: 1.0000 - val_loss: 3.7750 - val_acc: 0.8621\n",
      "Epoch 90/400\n",
      "58/58 [==============================] - 0s 411us/step - loss: 8.7439e-06 - acc: 1.0000 - val_loss: 3.8055 - val_acc: 0.8621\n",
      "Epoch 91/400\n",
      "58/58 [==============================] - 0s 410us/step - loss: 7.8269e-06 - acc: 1.0000 - val_loss: 3.8043 - val_acc: 0.8621\n",
      "Epoch 92/400\n",
      "58/58 [==============================] - 0s 415us/step - loss: 7.2745e-06 - acc: 1.0000 - val_loss: 3.8141 - val_acc: 0.8621\n",
      "Epoch 93/400\n",
      "58/58 [==============================] - 0s 404us/step - loss: 7.3792e-06 - acc: 1.0000 - val_loss: 3.8079 - val_acc: 0.8621\n",
      "Epoch 94/400\n",
      "58/58 [==============================] - 0s 404us/step - loss: 7.6547e-06 - acc: 1.0000 - val_loss: 3.8104 - val_acc: 0.8621\n",
      "Epoch 95/400\n",
      "58/58 [==============================] - 0s 402us/step - loss: 6.8298e-06 - acc: 1.0000 - val_loss: 3.8099 - val_acc: 0.8621\n",
      "Epoch 96/400\n",
      "58/58 [==============================] - 0s 411us/step - loss: 6.9293e-06 - acc: 1.0000 - val_loss: 3.8146 - val_acc: 0.8621\n",
      "Epoch 97/400\n",
      "58/58 [==============================] - 0s 409us/step - loss: 6.0265e-06 - acc: 1.0000 - val_loss: 3.8547 - val_acc: 0.8621\n",
      "Epoch 98/400\n",
      "58/58 [==============================] - 0s 406us/step - loss: 6.3074e-06 - acc: 1.0000 - val_loss: 3.8500 - val_acc: 0.8621\n",
      "Epoch 99/400\n",
      "58/58 [==============================] - 0s 443us/step - loss: 5.9063e-06 - acc: 1.0000 - val_loss: 3.8426 - val_acc: 0.8621\n",
      "Epoch 100/400\n",
      "58/58 [==============================] - 0s 407us/step - loss: 5.6772e-06 - acc: 1.0000 - val_loss: 3.8686 - val_acc: 0.8621\n",
      "Epoch 101/400\n",
      "58/58 [==============================] - 0s 411us/step - loss: 5.3497e-06 - acc: 1.0000 - val_loss: 3.8483 - val_acc: 0.8621\n",
      "Epoch 102/400\n",
      "58/58 [==============================] - 0s 398us/step - loss: 5.1714e-06 - acc: 1.0000 - val_loss: 3.8618 - val_acc: 0.8621\n",
      "Epoch 103/400\n",
      "58/58 [==============================] - 0s 400us/step - loss: 5.2647e-06 - acc: 1.0000 - val_loss: 3.8910 - val_acc: 0.8621\n",
      "Epoch 104/400\n",
      "58/58 [==============================] - 0s 403us/step - loss: 5.2346e-06 - acc: 1.0000 - val_loss: 3.9062 - val_acc: 0.8621\n",
      "Epoch 105/400\n",
      "58/58 [==============================] - 0s 406us/step - loss: 4.6200e-06 - acc: 1.0000 - val_loss: 3.8826 - val_acc: 0.8621\n",
      "Epoch 106/400\n",
      "58/58 [==============================] - 0s 414us/step - loss: 4.8086e-06 - acc: 1.0000 - val_loss: 3.9077 - val_acc: 0.8621\n",
      "Epoch 107/400\n",
      "58/58 [==============================] - 0s 409us/step - loss: 4.5744e-06 - acc: 1.0000 - val_loss: 3.9028 - val_acc: 0.8621\n",
      "Epoch 108/400\n",
      "58/58 [==============================] - 0s 403us/step - loss: 4.1639e-06 - acc: 1.0000 - val_loss: 3.9002 - val_acc: 0.8621\n",
      "Epoch 109/400\n",
      "58/58 [==============================] - 0s 408us/step - loss: 4.9381e-06 - acc: 1.0000 - val_loss: 3.9344 - val_acc: 0.8621\n",
      "Epoch 110/400\n",
      "58/58 [==============================] - 0s 407us/step - loss: 3.7731e-06 - acc: 1.0000 - val_loss: 3.9187 - val_acc: 0.8621\n",
      "Epoch 111/400\n",
      "58/58 [==============================] - 0s 406us/step - loss: 4.2965e-06 - acc: 1.0000 - val_loss: 3.9479 - val_acc: 0.8621\n",
      "Epoch 112/400\n",
      "58/58 [==============================] - 0s 402us/step - loss: 3.8498e-06 - acc: 1.0000 - val_loss: 3.9650 - val_acc: 0.8621\n",
      "Epoch 113/400\n",
      "58/58 [==============================] - 0s 414us/step - loss: 3.6612e-06 - acc: 1.0000 - val_loss: 3.9526 - val_acc: 0.8621\n",
      "Epoch 114/400\n",
      "58/58 [==============================] - 0s 409us/step - loss: 3.6353e-06 - acc: 1.0000 - val_loss: 3.9650 - val_acc: 0.8621\n",
      "Epoch 115/400\n",
      "58/58 [==============================] - 0s 424us/step - loss: 3.9286e-06 - acc: 1.0000 - val_loss: 3.9816 - val_acc: 0.8621\n",
      "Epoch 116/400\n",
      "58/58 [==============================] - 0s 407us/step - loss: 3.1978e-06 - acc: 1.0000 - val_loss: 3.9854 - val_acc: 0.8621\n",
      "Epoch 117/400\n",
      "58/58 [==============================] - 0s 411us/step - loss: 3.3015e-06 - acc: 1.0000 - val_loss: 3.9942 - val_acc: 0.8621\n",
      "Epoch 118/400\n",
      "58/58 [==============================] - 0s 422us/step - loss: 3.4321e-06 - acc: 1.0000 - val_loss: 4.0074 - val_acc: 0.8621\n",
      "Epoch 119/400\n",
      "58/58 [==============================] - 0s 415us/step - loss: 3.3222e-06 - acc: 1.0000 - val_loss: 4.0108 - val_acc: 0.8621\n",
      "Epoch 120/400\n",
      "58/58 [==============================] - 0s 410us/step - loss: 3.1937e-06 - acc: 1.0000 - val_loss: 4.0088 - val_acc: 0.8621\n",
      "Epoch 121/400\n",
      "58/58 [==============================] - 0s 403us/step - loss: 3.1336e-06 - acc: 1.0000 - val_loss: 4.0400 - val_acc: 0.8621\n",
      "Epoch 122/400\n",
      "58/58 [==============================] - 0s 407us/step - loss: 2.7407e-06 - acc: 1.0000 - val_loss: 4.0412 - val_acc: 0.8621\n",
      "Epoch 123/400\n",
      "58/58 [==============================] - 0s 418us/step - loss: 2.6495e-06 - acc: 1.0000 - val_loss: 4.0492 - val_acc: 0.8621\n",
      "Epoch 124/400\n",
      "58/58 [==============================] - 0s 539us/step - loss: 3.4010e-06 - acc: 1.0000 - val_loss: 4.0682 - val_acc: 0.8621\n",
      "Epoch 125/400\n",
      "58/58 [==============================] - 0s 408us/step - loss: 3.5440e-06 - acc: 1.0000 - val_loss: 4.0530 - val_acc: 0.8621\n",
      "Epoch 126/400\n",
      "58/58 [==============================] - 0s 408us/step - loss: 2.7946e-06 - acc: 1.0000 - val_loss: 4.0611 - val_acc: 0.8621\n",
      "Epoch 127/400\n",
      "58/58 [==============================] - 0s 416us/step - loss: 2.5210e-06 - acc: 1.0000 - val_loss: 4.0869 - val_acc: 0.8621\n",
      "Epoch 128/400\n",
      "58/58 [==============================] - 0s 415us/step - loss: 2.2639e-06 - acc: 1.0000 - val_loss: 4.0685 - val_acc: 0.8621\n",
      "Epoch 129/400\n",
      "58/58 [==============================] - 0s 408us/step - loss: 2.1333e-06 - acc: 1.0000 - val_loss: 4.1067 - val_acc: 0.8621\n",
      "Epoch 130/400\n",
      "58/58 [==============================] - 0s 408us/step - loss: 2.1830e-06 - acc: 1.0000 - val_loss: 4.1131 - val_acc: 0.8621\n",
      "Epoch 131/400\n",
      "58/58 [==============================] - 0s 405us/step - loss: 1.9187e-06 - acc: 1.0000 - val_loss: 4.1149 - val_acc: 0.8621\n",
      "Epoch 132/400\n",
      "58/58 [==============================] - 0s 412us/step - loss: 2.0296e-06 - acc: 1.0000 - val_loss: 4.1676 - val_acc: 0.8621\n",
      "Epoch 133/400\n",
      "58/58 [==============================] - 0s 405us/step - loss: 2.4328e-06 - acc: 1.0000 - val_loss: 4.1487 - val_acc: 0.8621\n",
      "Epoch 134/400\n",
      "58/58 [==============================] - 0s 405us/step - loss: 1.7352e-06 - acc: 1.0000 - val_loss: 4.1821 - val_acc: 0.8621\n",
      "Epoch 135/400\n",
      "58/58 [==============================] - 0s 411us/step - loss: 2.0835e-06 - acc: 1.0000 - val_loss: 4.2017 - val_acc: 0.8621\n",
      "Epoch 136/400\n",
      "58/58 [==============================] - 0s 416us/step - loss: 1.9052e-06 - acc: 1.0000 - val_loss: 4.1714 - val_acc: 0.8621\n",
      "Epoch 137/400\n",
      "58/58 [==============================] - 0s 417us/step - loss: 1.5528e-06 - acc: 1.0000 - val_loss: 4.1778 - val_acc: 0.8621\n",
      "Epoch 138/400\n",
      "58/58 [==============================] - 0s 408us/step - loss: 1.5632e-06 - acc: 1.0000 - val_loss: 4.2051 - val_acc: 0.8621\n",
      "Epoch 139/400\n",
      "58/58 [==============================] - 0s 414us/step - loss: 1.5663e-06 - acc: 1.0000 - val_loss: 4.1897 - val_acc: 0.8621\n",
      "Epoch 140/400\n",
      "58/58 [==============================] - 0s 407us/step - loss: 1.4626e-06 - acc: 1.0000 - val_loss: 4.2178 - val_acc: 0.8621\n",
      "Epoch 141/400\n",
      "58/58 [==============================] - 0s 550us/step - loss: 1.3652e-06 - acc: 1.0000 - val_loss: 4.2218 - val_acc: 0.8621\n",
      "Epoch 142/400\n",
      "58/58 [==============================] - 0s 458us/step - loss: 1.4471e-06 - acc: 1.0000 - val_loss: 4.2128 - val_acc: 0.8621\n",
      "Epoch 143/400\n",
      "58/58 [==============================] - 0s 408us/step - loss: 1.5787e-06 - acc: 1.0000 - val_loss: 4.2309 - val_acc: 0.8621\n",
      "Epoch 144/400\n",
      "58/58 [==============================] - 0s 416us/step - loss: 1.4253e-06 - acc: 1.0000 - val_loss: 4.2149 - val_acc: 0.8621\n",
      "Epoch 145/400\n",
      "58/58 [==============================] - 0s 414us/step - loss: 1.2418e-06 - acc: 1.0000 - val_loss: 4.2357 - val_acc: 0.8621\n",
      "Epoch 146/400\n",
      "58/58 [==============================] - 0s 409us/step - loss: 1.1382e-06 - acc: 1.0000 - val_loss: 4.2823 - val_acc: 0.8621\n",
      "Epoch 147/400\n",
      "58/58 [==============================] - 0s 408us/step - loss: 1.2377e-06 - acc: 1.0000 - val_loss: 4.2942 - val_acc: 0.8621\n",
      "Epoch 148/400\n",
      "58/58 [==============================] - 0s 407us/step - loss: 1.1071e-06 - acc: 1.0000 - val_loss: 4.2787 - val_acc: 0.8621\n",
      "Epoch 149/400\n",
      "58/58 [==============================] - 0s 407us/step - loss: 1.0293e-06 - acc: 1.0000 - val_loss: 4.3251 - val_acc: 0.8621\n",
      "Epoch 150/400\n",
      "58/58 [==============================] - 0s 411us/step - loss: 1.0159e-06 - acc: 1.0000 - val_loss: 4.3457 - val_acc: 0.8621\n",
      "Epoch 151/400\n",
      "58/58 [==============================] - 0s 411us/step - loss: 9.7854e-07 - acc: 1.0000 - val_loss: 4.3623 - val_acc: 0.8621\n",
      "Epoch 152/400\n",
      "58/58 [==============================] - 0s 412us/step - loss: 9.7025e-07 - acc: 1.0000 - val_loss: 4.2729 - val_acc: 0.8621\n",
      "Epoch 153/400\n",
      "58/58 [==============================] - 0s 415us/step - loss: 9.5367e-07 - acc: 1.0000 - val_loss: 4.3187 - val_acc: 0.8621\n",
      "Epoch 154/400\n",
      "58/58 [==============================] - 0s 415us/step - loss: 8.7696e-07 - acc: 1.0000 - val_loss: 4.3493 - val_acc: 0.8621\n",
      "Epoch 155/400\n",
      "58/58 [==============================] - 0s 408us/step - loss: 8.5726e-07 - acc: 1.0000 - val_loss: 4.3848 - val_acc: 0.8621\n",
      "Epoch 156/400\n",
      "58/58 [==============================] - 0s 412us/step - loss: 8.6452e-07 - acc: 1.0000 - val_loss: 4.3929 - val_acc: 0.8621\n",
      "Epoch 157/400\n",
      "58/58 [==============================] - 0s 416us/step - loss: 8.5830e-07 - acc: 1.0000 - val_loss: 4.4338 - val_acc: 0.8621\n",
      "Epoch 158/400\n",
      "58/58 [==============================] - 0s 408us/step - loss: 7.8678e-07 - acc: 1.0000 - val_loss: 4.3929 - val_acc: 0.8621\n",
      "Epoch 159/400\n",
      "58/58 [==============================] - 0s 411us/step - loss: 8.5415e-07 - acc: 1.0000 - val_loss: 4.3323 - val_acc: 0.8621\n",
      "Epoch 160/400\n",
      "58/58 [==============================] - 0s 405us/step - loss: 7.9403e-07 - acc: 1.0000 - val_loss: 4.3444 - val_acc: 0.8621\n",
      "Epoch 161/400\n",
      "58/58 [==============================] - 0s 412us/step - loss: 7.8781e-07 - acc: 1.0000 - val_loss: 4.4434 - val_acc: 0.8621\n",
      "Epoch 162/400\n",
      "58/58 [==============================] - 0s 406us/step - loss: 6.8934e-07 - acc: 1.0000 - val_loss: 4.4887 - val_acc: 0.8621\n",
      "Epoch 163/400\n",
      "58/58 [==============================] - 0s 416us/step - loss: 6.8934e-07 - acc: 1.0000 - val_loss: 4.4897 - val_acc: 0.8621\n",
      "Epoch 164/400\n",
      "58/58 [==============================] - 0s 415us/step - loss: 6.8934e-07 - acc: 1.0000 - val_loss: 4.4978 - val_acc: 0.8621\n",
      "Epoch 165/400\n",
      "58/58 [==============================] - 0s 406us/step - loss: 7.3495e-07 - acc: 1.0000 - val_loss: 4.5236 - val_acc: 0.8621\n",
      "Epoch 166/400\n",
      "58/58 [==============================] - 0s 403us/step - loss: 6.2299e-07 - acc: 1.0000 - val_loss: 4.5475 - val_acc: 0.8621\n",
      "Epoch 167/400\n",
      "58/58 [==============================] - 0s 403us/step - loss: 6.0123e-07 - acc: 1.0000 - val_loss: 4.5688 - val_acc: 0.8621\n",
      "Epoch 168/400\n",
      "58/58 [==============================] - 0s 414us/step - loss: 5.4214e-07 - acc: 1.0000 - val_loss: 4.5898 - val_acc: 0.8621\n",
      "Epoch 169/400\n",
      "58/58 [==============================] - 0s 411us/step - loss: 5.8982e-07 - acc: 1.0000 - val_loss: 4.6011 - val_acc: 0.8621\n",
      "Epoch 170/400\n",
      "58/58 [==============================] - 0s 416us/step - loss: 5.4940e-07 - acc: 1.0000 - val_loss: 4.6027 - val_acc: 0.8621\n",
      "Epoch 171/400\n",
      "58/58 [==============================] - 0s 405us/step - loss: 4.9238e-07 - acc: 1.0000 - val_loss: 4.6140 - val_acc: 0.8621\n",
      "Epoch 172/400\n",
      "58/58 [==============================] - 0s 412us/step - loss: 5.3799e-07 - acc: 1.0000 - val_loss: 4.6390 - val_acc: 0.8621\n",
      "Epoch 173/400\n",
      "58/58 [==============================] - 0s 409us/step - loss: 4.9757e-07 - acc: 1.0000 - val_loss: 4.5331 - val_acc: 0.8621\n",
      "Epoch 174/400\n",
      "58/58 [==============================] - 0s 420us/step - loss: 5.7946e-07 - acc: 1.0000 - val_loss: 4.5356 - val_acc: 0.8621\n",
      "Epoch 175/400\n",
      "58/58 [==============================] - 0s 404us/step - loss: 5.1104e-07 - acc: 1.0000 - val_loss: 4.5580 - val_acc: 0.8621\n",
      "Epoch 176/400\n",
      "58/58 [==============================] - 0s 412us/step - loss: 4.2501e-07 - acc: 1.0000 - val_loss: 4.5866 - val_acc: 0.8621\n",
      "Epoch 177/400\n",
      "58/58 [==============================] - 0s 416us/step - loss: 4.0738e-07 - acc: 1.0000 - val_loss: 4.6171 - val_acc: 0.8621\n",
      "Epoch 178/400\n",
      "58/58 [==============================] - 0s 415us/step - loss: 4.3744e-07 - acc: 1.0000 - val_loss: 4.6297 - val_acc: 0.8621\n",
      "Epoch 179/400\n",
      "58/58 [==============================] - 0s 411us/step - loss: 4.5714e-07 - acc: 1.0000 - val_loss: 4.6822 - val_acc: 0.8621\n",
      "Epoch 180/400\n",
      "58/58 [==============================] - 0s 408us/step - loss: 4.6440e-07 - acc: 1.0000 - val_loss: 4.6711 - val_acc: 0.8621\n",
      "Epoch 181/400\n",
      "58/58 [==============================] - 0s 410us/step - loss: 3.7836e-07 - acc: 1.0000 - val_loss: 4.6864 - val_acc: 0.8621\n",
      "Epoch 182/400\n",
      "58/58 [==============================] - 0s 411us/step - loss: 3.5866e-07 - acc: 1.0000 - val_loss: 4.6786 - val_acc: 0.8621\n",
      "Epoch 183/400\n",
      "58/58 [==============================] - 0s 409us/step - loss: 3.8665e-07 - acc: 1.0000 - val_loss: 4.7280 - val_acc: 0.8621\n",
      "Epoch 184/400\n",
      "58/58 [==============================] - 0s 402us/step - loss: 3.3379e-07 - acc: 1.0000 - val_loss: 4.6821 - val_acc: 0.8621\n",
      "Epoch 185/400\n",
      "58/58 [==============================] - 0s 413us/step - loss: 3.3275e-07 - acc: 1.0000 - val_loss: 4.7057 - val_acc: 0.8621\n",
      "Epoch 186/400\n",
      "58/58 [==============================] - 0s 406us/step - loss: 3.1616e-07 - acc: 1.0000 - val_loss: 4.7128 - val_acc: 0.8621\n",
      "Epoch 187/400\n",
      "58/58 [==============================] - 0s 417us/step - loss: 2.8610e-07 - acc: 1.0000 - val_loss: 4.7134 - val_acc: 0.8621\n",
      "Epoch 188/400\n",
      "58/58 [==============================] - 0s 548us/step - loss: 3.3482e-07 - acc: 1.0000 - val_loss: 4.7121 - val_acc: 0.8621\n",
      "Epoch 189/400\n",
      "58/58 [==============================] - 0s 412us/step - loss: 3.2342e-07 - acc: 1.0000 - val_loss: 4.7157 - val_acc: 0.8621\n",
      "Epoch 190/400\n",
      "58/58 [==============================] - 0s 413us/step - loss: 2.7263e-07 - acc: 1.0000 - val_loss: 4.7437 - val_acc: 0.8621\n",
      "Epoch 191/400\n",
      "58/58 [==============================] - 0s 405us/step - loss: 2.7055e-07 - acc: 1.0000 - val_loss: 4.7256 - val_acc: 0.8621\n",
      "Epoch 192/400\n",
      "58/58 [==============================] - 0s 668us/step - loss: 2.6537e-07 - acc: 1.0000 - val_loss: 4.7321 - val_acc: 0.8621\n",
      "Epoch 193/400\n",
      "58/58 [==============================] - 0s 407us/step - loss: 2.5500e-07 - acc: 1.0000 - val_loss: 4.7416 - val_acc: 0.8621\n",
      "Epoch 194/400\n",
      "58/58 [==============================] - 0s 405us/step - loss: 2.3634e-07 - acc: 1.0000 - val_loss: 4.7553 - val_acc: 0.8621\n",
      "Epoch 195/400\n",
      "58/58 [==============================] - 0s 410us/step - loss: 2.3634e-07 - acc: 1.0000 - val_loss: 4.7512 - val_acc: 0.8621\n",
      "Epoch 196/400\n",
      "58/58 [==============================] - 0s 420us/step - loss: 2.7055e-07 - acc: 1.0000 - val_loss: 4.7536 - val_acc: 0.8621\n",
      "Epoch 197/400\n",
      "58/58 [==============================] - 0s 411us/step - loss: 2.3220e-07 - acc: 1.0000 - val_loss: 4.7681 - val_acc: 0.8621\n",
      "Epoch 198/400\n",
      "58/58 [==============================] - 0s 414us/step - loss: 2.1147e-07 - acc: 1.0000 - val_loss: 4.7880 - val_acc: 0.8621\n",
      "Epoch 199/400\n",
      "58/58 [==============================] - 0s 431us/step - loss: 2.1458e-07 - acc: 1.0000 - val_loss: 4.7698 - val_acc: 0.8621\n",
      "Epoch 200/400\n",
      "58/58 [==============================] - 0s 412us/step - loss: 2.1458e-07 - acc: 1.0000 - val_loss: 4.7423 - val_acc: 0.8621\n",
      "Epoch 201/400\n",
      "58/58 [==============================] - 0s 410us/step - loss: 2.0317e-07 - acc: 1.0000 - val_loss: 4.7643 - val_acc: 0.8621\n",
      "Epoch 202/400\n",
      "58/58 [==============================] - 0s 404us/step - loss: 1.9177e-07 - acc: 1.0000 - val_loss: 4.7202 - val_acc: 0.8621\n",
      "Epoch 203/400\n",
      "58/58 [==============================] - 0s 417us/step - loss: 1.8244e-07 - acc: 1.0000 - val_loss: 4.7439 - val_acc: 0.8621\n",
      "Epoch 204/400\n",
      "58/58 [==============================] - 0s 409us/step - loss: 1.8348e-07 - acc: 1.0000 - val_loss: 4.7213 - val_acc: 0.8621\n",
      "Epoch 205/400\n",
      "58/58 [==============================] - 0s 413us/step - loss: 1.6586e-07 - acc: 1.0000 - val_loss: 4.6647 - val_acc: 0.8621\n",
      "Epoch 206/400\n",
      "58/58 [==============================] - 0s 412us/step - loss: 1.8037e-07 - acc: 1.0000 - val_loss: 4.6306 - val_acc: 0.8621\n",
      "Epoch 207/400\n",
      "58/58 [==============================] - 0s 410us/step - loss: 1.6689e-07 - acc: 1.0000 - val_loss: 4.6791 - val_acc: 0.8621\n",
      "Epoch 208/400\n",
      "58/58 [==============================] - 0s 411us/step - loss: 1.7519e-07 - acc: 1.0000 - val_loss: 4.7217 - val_acc: 0.8621\n",
      "Epoch 209/400\n",
      "58/58 [==============================] - 0s 413us/step - loss: 1.4098e-07 - acc: 1.0000 - val_loss: 4.6616 - val_acc: 0.8621\n",
      "Epoch 210/400\n",
      "58/58 [==============================] - 0s 414us/step - loss: 1.4927e-07 - acc: 1.0000 - val_loss: 4.7167 - val_acc: 0.8621\n",
      "Epoch 211/400\n",
      "58/58 [==============================] - 0s 412us/step - loss: 1.8037e-07 - acc: 1.0000 - val_loss: 4.6189 - val_acc: 0.8621\n",
      "Epoch 212/400\n",
      "58/58 [==============================] - 0s 412us/step - loss: 1.4616e-07 - acc: 1.0000 - val_loss: 4.7162 - val_acc: 0.8621\n",
      "Epoch 213/400\n",
      "58/58 [==============================] - 0s 412us/step - loss: 1.7519e-07 - acc: 1.0000 - val_loss: 4.6544 - val_acc: 0.8621\n",
      "Epoch 214/400\n",
      "58/58 [==============================] - 0s 411us/step - loss: 2.1561e-07 - acc: 1.0000 - val_loss: 4.7556 - val_acc: 0.8621\n",
      "Epoch 215/400\n",
      "58/58 [==============================] - 0s 410us/step - loss: 1.3165e-07 - acc: 1.0000 - val_loss: 4.6908 - val_acc: 0.8621\n",
      "Epoch 216/400\n",
      "58/58 [==============================] - 0s 408us/step - loss: 1.3165e-07 - acc: 1.0000 - val_loss: 4.7407 - val_acc: 0.8621\n",
      "Epoch 217/400\n",
      "58/58 [==============================] - 0s 417us/step - loss: 1.1403e-07 - acc: 1.0000 - val_loss: 4.7205 - val_acc: 0.8621\n",
      "Epoch 218/400\n",
      "58/58 [==============================] - 0s 409us/step - loss: 1.1092e-07 - acc: 1.0000 - val_loss: 4.6672 - val_acc: 0.8621\n",
      "Epoch 219/400\n",
      "58/58 [==============================] - 0s 410us/step - loss: 1.0781e-07 - acc: 1.0000 - val_loss: 4.6628 - val_acc: 0.8621\n",
      "Epoch 220/400\n",
      "58/58 [==============================] - 0s 410us/step - loss: 1.0055e-07 - acc: 1.0000 - val_loss: 4.7819 - val_acc: 0.8621\n",
      "Epoch 221/400\n",
      "58/58 [==============================] - 0s 413us/step - loss: 1.0159e-07 - acc: 1.0000 - val_loss: 4.7310 - val_acc: 0.8621\n",
      "Epoch 222/400\n",
      "58/58 [==============================] - 0s 416us/step - loss: 1.0573e-07 - acc: 1.0000 - val_loss: 4.7387 - val_acc: 0.8621\n",
      "Epoch 223/400\n",
      "58/58 [==============================] - 0s 408us/step - loss: 1.0470e-07 - acc: 1.0000 - val_loss: 4.6346 - val_acc: 0.8621\n",
      "Epoch 224/400\n",
      "58/58 [==============================] - 0s 402us/step - loss: 9.4331e-08 - acc: 1.0000 - val_loss: 4.7556 - val_acc: 0.8621\n",
      "Epoch 225/400\n",
      "58/58 [==============================] - 0s 409us/step - loss: 8.6038e-08 - acc: 1.0000 - val_loss: 4.7437 - val_acc: 0.8621\n",
      "Epoch 226/400\n",
      "58/58 [==============================] - 0s 414us/step - loss: 7.8782e-08 - acc: 1.0000 - val_loss: 4.6904 - val_acc: 0.8621\n",
      "Epoch 227/400\n",
      "58/58 [==============================] - 0s 407us/step - loss: 8.1892e-08 - acc: 1.0000 - val_loss: 4.7890 - val_acc: 0.8621\n",
      "Epoch 228/400\n",
      "58/58 [==============================] - 0s 406us/step - loss: 8.3965e-08 - acc: 1.0000 - val_loss: 4.7461 - val_acc: 0.8621\n",
      "Epoch 229/400\n",
      "58/58 [==============================] - 0s 484us/step - loss: 7.2562e-08 - acc: 1.0000 - val_loss: 4.7192 - val_acc: 0.8621\n",
      "Epoch 230/400\n",
      "58/58 [==============================] - 0s 379us/step - loss: 7.6709e-08 - acc: 1.0000 - val_loss: 4.7024 - val_acc: 0.8621\n",
      "Epoch 231/400\n",
      "58/58 [==============================] - 0s 416us/step - loss: 7.7745e-08 - acc: 1.0000 - val_loss: 4.7281 - val_acc: 0.8621\n",
      "Epoch 232/400\n",
      "58/58 [==============================] - 0s 418us/step - loss: 6.6343e-08 - acc: 1.0000 - val_loss: 4.7750 - val_acc: 0.8621\n",
      "Epoch 233/400\n",
      "58/58 [==============================] - 0s 409us/step - loss: 6.3233e-08 - acc: 1.0000 - val_loss: 4.7565 - val_acc: 0.8621\n",
      "Epoch 234/400\n",
      "58/58 [==============================] - 0s 414us/step - loss: 6.3233e-08 - acc: 1.0000 - val_loss: 4.6980 - val_acc: 0.8621\n",
      "Epoch 235/400\n",
      "58/58 [==============================] - 0s 415us/step - loss: 6.2196e-08 - acc: 1.0000 - val_loss: 4.8398 - val_acc: 0.8621\n",
      "Epoch 236/400\n",
      "58/58 [==============================] - 0s 650us/step - loss: 5.9086e-08 - acc: 1.0000 - val_loss: 4.7870 - val_acc: 0.8276\n",
      "Epoch 237/400\n",
      "58/58 [==============================] - 0s 414us/step - loss: 5.5977e-08 - acc: 1.0000 - val_loss: 4.7507 - val_acc: 0.8276\n",
      "Epoch 238/400\n",
      "58/58 [==============================] - 0s 415us/step - loss: 6.4269e-08 - acc: 1.0000 - val_loss: 4.9232 - val_acc: 0.8276\n",
      "Epoch 239/400\n",
      "58/58 [==============================] - 0s 411us/step - loss: 6.1160e-08 - acc: 1.0000 - val_loss: 4.8701 - val_acc: 0.8276\n",
      "Epoch 240/400\n",
      "58/58 [==============================] - 0s 416us/step - loss: 6.9452e-08 - acc: 1.0000 - val_loss: 4.7820 - val_acc: 0.8276\n",
      "Epoch 241/400\n",
      "58/58 [==============================] - 0s 409us/step - loss: 5.5977e-08 - acc: 1.0000 - val_loss: 4.7494 - val_acc: 0.8276\n",
      "Epoch 242/400\n",
      "58/58 [==============================] - 0s 549us/step - loss: 5.2867e-08 - acc: 1.0000 - val_loss: 4.8942 - val_acc: 0.8276\n",
      "Epoch 243/400\n",
      "58/58 [==============================] - 0s 409us/step - loss: 5.0793e-08 - acc: 1.0000 - val_loss: 4.8602 - val_acc: 0.8276\n",
      "Epoch 244/400\n",
      "58/58 [==============================] - 0s 411us/step - loss: 5.1830e-08 - acc: 1.0000 - val_loss: 4.8102 - val_acc: 0.8276\n",
      "Epoch 245/400\n",
      "58/58 [==============================] - 0s 410us/step - loss: 4.7684e-08 - acc: 1.0000 - val_loss: 4.7724 - val_acc: 0.8276\n",
      "Epoch 246/400\n",
      "58/58 [==============================] - 0s 413us/step - loss: 5.2867e-08 - acc: 1.0000 - val_loss: 4.9427 - val_acc: 0.8276\n",
      "Epoch 247/400\n",
      "58/58 [==============================] - 0s 410us/step - loss: 3.8354e-08 - acc: 1.0000 - val_loss: 4.8964 - val_acc: 0.8276\n",
      "Epoch 248/400\n",
      "58/58 [==============================] - 0s 410us/step - loss: 4.8720e-08 - acc: 1.0000 - val_loss: 4.8501 - val_acc: 0.8276\n",
      "Epoch 249/400\n",
      "58/58 [==============================] - 0s 398us/step - loss: 3.9391e-08 - acc: 1.0000 - val_loss: 4.8095 - val_acc: 0.8276\n",
      "Epoch 250/400\n",
      "58/58 [==============================] - 0s 416us/step - loss: 3.9391e-08 - acc: 1.0000 - val_loss: 5.0216 - val_acc: 0.8276\n",
      "Epoch 251/400\n",
      "58/58 [==============================] - 0s 410us/step - loss: 3.7318e-08 - acc: 1.0000 - val_loss: 4.9470 - val_acc: 0.8276\n",
      "Epoch 252/400\n",
      "58/58 [==============================] - 0s 413us/step - loss: 3.7318e-08 - acc: 1.0000 - val_loss: 4.8527 - val_acc: 0.8276\n",
      "Epoch 253/400\n",
      "58/58 [==============================] - 0s 416us/step - loss: 4.1464e-08 - acc: 1.0000 - val_loss: 4.7851 - val_acc: 0.8276\n",
      "Epoch 254/400\n",
      "58/58 [==============================] - 0s 414us/step - loss: 3.7318e-08 - acc: 1.0000 - val_loss: 4.7322 - val_acc: 0.8276\n",
      "Epoch 255/400\n",
      "58/58 [==============================] - 0s 409us/step - loss: 3.5244e-08 - acc: 1.0000 - val_loss: 4.8626 - val_acc: 0.8276\n",
      "Epoch 256/400\n",
      "58/58 [==============================] - 0s 409us/step - loss: 3.8354e-08 - acc: 1.0000 - val_loss: 4.9396 - val_acc: 0.8276\n",
      "Epoch 257/400\n",
      "58/58 [==============================] - 0s 415us/step - loss: 3.1098e-08 - acc: 1.0000 - val_loss: 4.8251 - val_acc: 0.8276\n",
      "Epoch 258/400\n",
      "58/58 [==============================] - 0s 407us/step - loss: 2.9025e-08 - acc: 1.0000 - val_loss: 4.7069 - val_acc: 0.8276\n",
      "Epoch 259/400\n",
      "58/58 [==============================] - 0s 412us/step - loss: 3.5244e-08 - acc: 1.0000 - val_loss: 5.3464 - val_acc: 0.8276\n",
      "Epoch 260/400\n",
      "58/58 [==============================] - 0s 453us/step - loss: 2.9025e-08 - acc: 1.0000 - val_loss: 5.2754 - val_acc: 0.8276\n",
      "Epoch 261/400\n",
      "58/58 [==============================] - 0s 417us/step - loss: 2.9025e-08 - acc: 1.0000 - val_loss: 5.2330 - val_acc: 0.8276\n",
      "Epoch 262/400\n",
      "58/58 [==============================] - 0s 411us/step - loss: 2.6952e-08 - acc: 1.0000 - val_loss: 5.1236 - val_acc: 0.8276\n",
      "Epoch 263/400\n",
      "58/58 [==============================] - 0s 409us/step - loss: 3.0061e-08 - acc: 1.0000 - val_loss: 5.0662 - val_acc: 0.8276\n",
      "Epoch 264/400\n",
      "58/58 [==============================] - 0s 405us/step - loss: 2.2805e-08 - acc: 1.0000 - val_loss: 4.9865 - val_acc: 0.8276\n",
      "Epoch 265/400\n",
      "58/58 [==============================] - 0s 416us/step - loss: 2.6952e-08 - acc: 1.0000 - val_loss: 4.9333 - val_acc: 0.8276\n",
      "Epoch 266/400\n",
      "58/58 [==============================] - 0s 406us/step - loss: 2.3842e-08 - acc: 1.0000 - val_loss: 4.8552 - val_acc: 0.8276\n",
      "Epoch 267/400\n",
      "58/58 [==============================] - 0s 409us/step - loss: 2.2805e-08 - acc: 1.0000 - val_loss: 4.8560 - val_acc: 0.8276\n",
      "Epoch 268/400\n",
      "58/58 [==============================] - 0s 404us/step - loss: 2.2805e-08 - acc: 1.0000 - val_loss: 4.7374 - val_acc: 0.8276\n",
      "Epoch 269/400\n",
      "58/58 [==============================] - 0s 412us/step - loss: 2.5915e-08 - acc: 1.0000 - val_loss: 4.8123 - val_acc: 0.8276\n",
      "Epoch 270/400\n",
      "58/58 [==============================] - 0s 413us/step - loss: 3.0061e-08 - acc: 1.0000 - val_loss: 4.9984 - val_acc: 0.8276\n",
      "Epoch 271/400\n",
      "58/58 [==============================] - 0s 415us/step - loss: 2.4878e-08 - acc: 1.0000 - val_loss: 4.8570 - val_acc: 0.8276\n",
      "Epoch 272/400\n",
      "58/58 [==============================] - 0s 416us/step - loss: 2.4878e-08 - acc: 1.0000 - val_loss: 4.8164 - val_acc: 0.8276\n",
      "Epoch 273/400\n",
      "58/58 [==============================] - 0s 413us/step - loss: 1.9695e-08 - acc: 1.0000 - val_loss: 4.6890 - val_acc: 0.8276\n",
      "Epoch 274/400\n",
      "58/58 [==============================] - 0s 419us/step - loss: 2.0732e-08 - acc: 1.0000 - val_loss: 5.4041 - val_acc: 0.8276\n",
      "Epoch 275/400\n",
      "58/58 [==============================] - 0s 656us/step - loss: 2.2805e-08 - acc: 1.0000 - val_loss: 4.9433 - val_acc: 0.8276\n",
      "Epoch 276/400\n",
      "58/58 [==============================] - 0s 409us/step - loss: 2.0732e-08 - acc: 1.0000 - val_loss: 5.0711 - val_acc: 0.8276\n",
      "Epoch 277/400\n",
      "58/58 [==============================] - 0s 415us/step - loss: 1.8659e-08 - acc: 1.0000 - val_loss: 5.0122 - val_acc: 0.8276\n",
      "Epoch 278/400\n",
      "58/58 [==============================] - 0s 410us/step - loss: 1.9695e-08 - acc: 1.0000 - val_loss: 4.9645 - val_acc: 0.8276\n",
      "Epoch 279/400\n",
      "58/58 [==============================] - 0s 405us/step - loss: 1.7622e-08 - acc: 1.0000 - val_loss: 4.9010 - val_acc: 0.8276\n",
      "Epoch 280/400\n",
      "58/58 [==============================] - 0s 412us/step - loss: 1.7622e-08 - acc: 1.0000 - val_loss: 4.8331 - val_acc: 0.8276\n",
      "Epoch 281/400\n",
      "58/58 [==============================] - 0s 409us/step - loss: 1.4512e-08 - acc: 1.0000 - val_loss: 4.8343 - val_acc: 0.8276\n",
      "Epoch 282/400\n",
      "58/58 [==============================] - 0s 406us/step - loss: 1.8659e-08 - acc: 1.0000 - val_loss: 4.7893 - val_acc: 0.8276\n",
      "Epoch 283/400\n",
      "58/58 [==============================] - 0s 412us/step - loss: 1.1403e-08 - acc: 1.0000 - val_loss: 4.6850 - val_acc: 0.8276\n",
      "Epoch 284/400\n",
      "58/58 [==============================] - 0s 408us/step - loss: 1.2439e-08 - acc: 1.0000 - val_loss: 4.5805 - val_acc: 0.8276\n",
      "Epoch 285/400\n",
      "58/58 [==============================] - 0s 413us/step - loss: 1.3476e-08 - acc: 1.0000 - val_loss: 5.5562 - val_acc: 0.8276\n",
      "Epoch 286/400\n",
      "58/58 [==============================] - 0s 420us/step - loss: 5.8050e-08 - acc: 1.0000 - val_loss: 4.4844 - val_acc: 0.8276\n",
      "Epoch 287/400\n",
      "58/58 [==============================] - 0s 561us/step - loss: 1.5549e-07 - acc: 1.0000 - val_loss: 9.0805 - val_acc: 0.8276\n",
      "Epoch 288/400\n",
      "58/58 [==============================] - 0s 410us/step - loss: 1.2674 - acc: 0.9652 - val_loss: 2.2600 - val_acc: 0.8276\n",
      "Epoch 289/400\n",
      "58/58 [==============================] - 0s 402us/step - loss: 0.3298 - acc: 0.9652 - val_loss: 2.9897 - val_acc: 0.8621\n",
      "Epoch 290/400\n",
      "58/58 [==============================] - 0s 400us/step - loss: 0.2960 - acc: 0.9478 - val_loss: 2.8406 - val_acc: 0.8276\n",
      "Epoch 291/400\n",
      "58/58 [==============================] - 0s 411us/step - loss: 0.1855 - acc: 0.9739 - val_loss: 2.7800 - val_acc: 0.7931\n",
      "Epoch 292/400\n",
      "58/58 [==============================] - 0s 404us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 2.6006 - val_acc: 0.7931\n",
      "Epoch 293/400\n",
      "58/58 [==============================] - 0s 415us/step - loss: 3.1831e-05 - acc: 1.0000 - val_loss: 2.5895 - val_acc: 0.7931\n",
      "Epoch 294/400\n",
      "58/58 [==============================] - 0s 416us/step - loss: 2.9135e-05 - acc: 1.0000 - val_loss: 2.5917 - val_acc: 0.7931\n",
      "Epoch 295/400\n",
      "58/58 [==============================] - 0s 415us/step - loss: 2.1847e-05 - acc: 1.0000 - val_loss: 2.5927 - val_acc: 0.7931\n",
      "Epoch 296/400\n",
      "58/58 [==============================] - 0s 416us/step - loss: 1.9625e-05 - acc: 1.0000 - val_loss: 2.5935 - val_acc: 0.7931\n",
      "Epoch 297/400\n",
      "58/58 [==============================] - 0s 411us/step - loss: 1.8041e-05 - acc: 1.0000 - val_loss: 2.5944 - val_acc: 0.7931\n",
      "Epoch 298/400\n",
      "58/58 [==============================] - 0s 415us/step - loss: 1.6797e-05 - acc: 1.0000 - val_loss: 2.5952 - val_acc: 0.7931\n",
      "Epoch 299/400\n",
      "58/58 [==============================] - 0s 410us/step - loss: 1.5866e-05 - acc: 1.0000 - val_loss: 2.5959 - val_acc: 0.7931\n",
      "Epoch 300/400\n",
      "58/58 [==============================] - 0s 416us/step - loss: 1.4914e-05 - acc: 1.0000 - val_loss: 2.5971 - val_acc: 0.7931\n",
      "Epoch 301/400\n",
      "58/58 [==============================] - 0s 416us/step - loss: 1.4175e-05 - acc: 1.0000 - val_loss: 2.5980 - val_acc: 0.7931\n",
      "Epoch 302/400\n",
      "58/58 [==============================] - 0s 413us/step - loss: 1.3523e-05 - acc: 1.0000 - val_loss: 2.5995 - val_acc: 0.7931\n",
      "Epoch 303/400\n",
      "58/58 [==============================] - 0s 407us/step - loss: 1.2872e-05 - acc: 1.0000 - val_loss: 2.6004 - val_acc: 0.7931\n",
      "Epoch 304/400\n",
      "58/58 [==============================] - 0s 414us/step - loss: 1.2330e-05 - acc: 1.0000 - val_loss: 2.6014 - val_acc: 0.7931\n",
      "Epoch 305/400\n",
      "58/58 [==============================] - 0s 413us/step - loss: 1.1799e-05 - acc: 1.0000 - val_loss: 2.6021 - val_acc: 0.7931\n",
      "Epoch 306/400\n",
      "58/58 [==============================] - 0s 406us/step - loss: 1.1322e-05 - acc: 1.0000 - val_loss: 2.6031 - val_acc: 0.7931\n",
      "Epoch 307/400\n",
      "58/58 [==============================] - 0s 402us/step - loss: 1.0877e-05 - acc: 1.0000 - val_loss: 2.6038 - val_acc: 0.7931\n",
      "Epoch 308/400\n",
      "58/58 [==============================] - 0s 415us/step - loss: 1.0473e-05 - acc: 1.0000 - val_loss: 2.6048 - val_acc: 0.7931\n",
      "Epoch 309/400\n",
      "58/58 [==============================] - 0s 412us/step - loss: 1.0073e-05 - acc: 1.0000 - val_loss: 2.6056 - val_acc: 0.7931\n",
      "Epoch 310/400\n",
      "58/58 [==============================] - 0s 664us/step - loss: 9.7114e-06 - acc: 1.0000 - val_loss: 2.6064 - val_acc: 0.7931\n",
      "Epoch 311/400\n",
      "58/58 [==============================] - 0s 412us/step - loss: 9.3726e-06 - acc: 1.0000 - val_loss: 2.6071 - val_acc: 0.7931\n",
      "Epoch 312/400\n",
      "58/58 [==============================] - 0s 409us/step - loss: 9.0442e-06 - acc: 1.0000 - val_loss: 2.6079 - val_acc: 0.7931\n",
      "Epoch 313/400\n",
      "58/58 [==============================] - 0s 409us/step - loss: 8.7303e-06 - acc: 1.0000 - val_loss: 2.6087 - val_acc: 0.7931\n",
      "Epoch 314/400\n",
      "58/58 [==============================] - 0s 414us/step - loss: 8.4401e-06 - acc: 1.0000 - val_loss: 2.6094 - val_acc: 0.7931\n",
      "Epoch 315/400\n",
      "58/58 [==============================] - 0s 403us/step - loss: 8.1562e-06 - acc: 1.0000 - val_loss: 2.6104 - val_acc: 0.7931\n",
      "Epoch 316/400\n",
      "58/58 [==============================] - 0s 410us/step - loss: 7.8889e-06 - acc: 1.0000 - val_loss: 2.6108 - val_acc: 0.7931\n",
      "Epoch 317/400\n",
      "58/58 [==============================] - 0s 537us/step - loss: 7.6174e-06 - acc: 1.0000 - val_loss: 2.6122 - val_acc: 0.7931\n",
      "Epoch 318/400\n",
      "58/58 [==============================] - 0s 421us/step - loss: 7.3615e-06 - acc: 1.0000 - val_loss: 2.6129 - val_acc: 0.7931\n",
      "Epoch 319/400\n",
      "58/58 [==============================] - 0s 412us/step - loss: 7.1501e-06 - acc: 1.0000 - val_loss: 2.6137 - val_acc: 0.7931\n",
      "Epoch 320/400\n",
      "58/58 [==============================] - 0s 413us/step - loss: 6.9315e-06 - acc: 1.0000 - val_loss: 2.6143 - val_acc: 0.7931\n",
      "Epoch 321/400\n",
      "58/58 [==============================] - 0s 414us/step - loss: 6.7253e-06 - acc: 1.0000 - val_loss: 2.6151 - val_acc: 0.7931\n",
      "Epoch 322/400\n",
      "58/58 [==============================] - 0s 411us/step - loss: 6.5315e-06 - acc: 1.0000 - val_loss: 2.6155 - val_acc: 0.7931\n",
      "Epoch 323/400\n",
      "58/58 [==============================] - 0s 420us/step - loss: 6.3419e-06 - acc: 1.0000 - val_loss: 2.6158 - val_acc: 0.7931\n",
      "Epoch 324/400\n",
      "58/58 [==============================] - 0s 412us/step - loss: 6.2062e-06 - acc: 1.0000 - val_loss: 2.6165 - val_acc: 0.7931\n",
      "Epoch 325/400\n",
      "58/58 [==============================] - 0s 404us/step - loss: 5.9844e-06 - acc: 1.0000 - val_loss: 2.6170 - val_acc: 0.7931\n",
      "Epoch 326/400\n",
      "58/58 [==============================] - 0s 543us/step - loss: 5.8155e-06 - acc: 1.0000 - val_loss: 2.6175 - val_acc: 0.7931\n",
      "Epoch 327/400\n",
      "58/58 [==============================] - 0s 407us/step - loss: 5.6549e-06 - acc: 1.0000 - val_loss: 2.6180 - val_acc: 0.7931\n",
      "Epoch 328/400\n",
      "58/58 [==============================] - 0s 410us/step - loss: 5.4994e-06 - acc: 1.0000 - val_loss: 2.6183 - val_acc: 0.7931\n",
      "Epoch 329/400\n",
      "58/58 [==============================] - 0s 419us/step - loss: 5.3430e-06 - acc: 1.0000 - val_loss: 2.6187 - val_acc: 0.7931\n",
      "Epoch 330/400\n",
      "58/58 [==============================] - 0s 405us/step - loss: 5.2145e-06 - acc: 1.0000 - val_loss: 2.6195 - val_acc: 0.7931\n",
      "Epoch 331/400\n",
      "58/58 [==============================] - 0s 409us/step - loss: 5.0476e-06 - acc: 1.0000 - val_loss: 2.6200 - val_acc: 0.7931\n",
      "Epoch 332/400\n",
      "58/58 [==============================] - 0s 419us/step - loss: 4.9378e-06 - acc: 1.0000 - val_loss: 2.6202 - val_acc: 0.7931\n",
      "Epoch 333/400\n",
      "58/58 [==============================] - 0s 413us/step - loss: 4.8466e-06 - acc: 1.0000 - val_loss: 2.6208 - val_acc: 0.7931\n",
      "Epoch 334/400\n",
      "58/58 [==============================] - 0s 414us/step - loss: 4.7595e-06 - acc: 1.0000 - val_loss: 2.6213 - val_acc: 0.7931\n",
      "Epoch 335/400\n",
      "58/58 [==============================] - 0s 417us/step - loss: 4.6642e-06 - acc: 1.0000 - val_loss: 2.6215 - val_acc: 0.7931\n",
      "Epoch 336/400\n",
      "58/58 [==============================] - 0s 407us/step - loss: 4.5802e-06 - acc: 1.0000 - val_loss: 2.6221 - val_acc: 0.7931\n",
      "Epoch 337/400\n",
      "58/58 [==============================] - 0s 412us/step - loss: 4.4901e-06 - acc: 1.0000 - val_loss: 2.6226 - val_acc: 0.7931\n",
      "Epoch 338/400\n",
      "58/58 [==============================] - 0s 415us/step - loss: 4.4061e-06 - acc: 1.0000 - val_loss: 2.6230 - val_acc: 0.7931\n",
      "Epoch 339/400\n",
      "58/58 [==============================] - 0s 414us/step - loss: 4.3263e-06 - acc: 1.0000 - val_loss: 2.6234 - val_acc: 0.7931\n",
      "Epoch 340/400\n",
      "58/58 [==============================] - 0s 412us/step - loss: 4.2403e-06 - acc: 1.0000 - val_loss: 2.6236 - val_acc: 0.7931\n",
      "Epoch 341/400\n",
      "58/58 [==============================] - 0s 400us/step - loss: 4.1232e-06 - acc: 1.0000 - val_loss: 2.6248 - val_acc: 0.7931\n",
      "Epoch 342/400\n",
      "58/58 [==============================] - 0s 675us/step - loss: 4.0330e-06 - acc: 1.0000 - val_loss: 2.6251 - val_acc: 0.7931\n",
      "Epoch 343/400\n",
      "58/58 [==============================] - 0s 403us/step - loss: 3.9657e-06 - acc: 1.0000 - val_loss: 2.6255 - val_acc: 0.7931\n",
      "Epoch 344/400\n",
      "58/58 [==============================] - 0s 412us/step - loss: 3.8931e-06 - acc: 1.0000 - val_loss: 2.6258 - val_acc: 0.7931\n",
      "Epoch 345/400\n",
      "58/58 [==============================] - 0s 414us/step - loss: 3.8237e-06 - acc: 1.0000 - val_loss: 2.6261 - val_acc: 0.7931\n",
      "Epoch 346/400\n",
      "58/58 [==============================] - 0s 409us/step - loss: 3.7615e-06 - acc: 1.0000 - val_loss: 2.6264 - val_acc: 0.7931\n",
      "Epoch 347/400\n",
      "58/58 [==============================] - 0s 410us/step - loss: 3.6931e-06 - acc: 1.0000 - val_loss: 2.6268 - val_acc: 0.7931\n",
      "Epoch 348/400\n",
      "58/58 [==============================] - 0s 419us/step - loss: 3.6247e-06 - acc: 1.0000 - val_loss: 2.6270 - val_acc: 0.7931\n",
      "Epoch 349/400\n",
      "58/58 [==============================] - 0s 413us/step - loss: 3.5480e-06 - acc: 1.0000 - val_loss: 2.6275 - val_acc: 0.7931\n",
      "Epoch 350/400\n",
      "58/58 [==============================] - 0s 408us/step - loss: 3.4858e-06 - acc: 1.0000 - val_loss: 2.6280 - val_acc: 0.7931\n",
      "Epoch 351/400\n",
      "58/58 [==============================] - 0s 424us/step - loss: 3.4226e-06 - acc: 1.0000 - val_loss: 2.6283 - val_acc: 0.7931\n",
      "Epoch 352/400\n",
      "58/58 [==============================] - 0s 407us/step - loss: 3.3625e-06 - acc: 1.0000 - val_loss: 2.6286 - val_acc: 0.7931\n",
      "Epoch 353/400\n",
      "58/58 [==============================] - 0s 412us/step - loss: 3.2993e-06 - acc: 1.0000 - val_loss: 2.6290 - val_acc: 0.7931\n",
      "Epoch 354/400\n",
      "58/58 [==============================] - 0s 407us/step - loss: 3.2423e-06 - acc: 1.0000 - val_loss: 2.6295 - val_acc: 0.7931\n",
      "Epoch 355/400\n",
      "58/58 [==============================] - 0s 420us/step - loss: 3.1790e-06 - acc: 1.0000 - val_loss: 2.6298 - val_acc: 0.7931\n",
      "Epoch 356/400\n",
      "58/58 [==============================] - 0s 412us/step - loss: 3.1220e-06 - acc: 1.0000 - val_loss: 2.6301 - val_acc: 0.7931\n",
      "Epoch 357/400\n",
      "58/58 [==============================] - 0s 404us/step - loss: 3.0650e-06 - acc: 1.0000 - val_loss: 2.6306 - val_acc: 0.7931\n",
      "Epoch 358/400\n",
      "58/58 [==============================] - 0s 411us/step - loss: 3.0060e-06 - acc: 1.0000 - val_loss: 2.6308 - val_acc: 0.7931\n",
      "Epoch 359/400\n",
      "58/58 [==============================] - 0s 408us/step - loss: 2.9531e-06 - acc: 1.0000 - val_loss: 2.6314 - val_acc: 0.7931\n",
      "Epoch 360/400\n",
      "58/58 [==============================] - 0s 447us/step - loss: 2.8940e-06 - acc: 1.0000 - val_loss: 2.6317 - val_acc: 0.7931\n",
      "Epoch 361/400\n",
      "58/58 [==============================] - 0s 412us/step - loss: 2.8401e-06 - acc: 1.0000 - val_loss: 2.6321 - val_acc: 0.7931\n",
      "Epoch 362/400\n",
      "58/58 [==============================] - 0s 521us/step - loss: 2.7852e-06 - acc: 1.0000 - val_loss: 2.6322 - val_acc: 0.7931\n",
      "Epoch 363/400\n",
      "58/58 [==============================] - 0s 397us/step - loss: 2.7344e-06 - acc: 1.0000 - val_loss: 2.6325 - val_acc: 0.7931\n",
      "Epoch 364/400\n",
      "58/58 [==============================] - 0s 415us/step - loss: 2.6826e-06 - acc: 1.0000 - val_loss: 2.6331 - val_acc: 0.7931\n",
      "Epoch 365/400\n",
      "58/58 [==============================] - 0s 414us/step - loss: 2.6349e-06 - acc: 1.0000 - val_loss: 2.6337 - val_acc: 0.7931\n",
      "Epoch 366/400\n",
      "58/58 [==============================] - 0s 413us/step - loss: 2.5841e-06 - acc: 1.0000 - val_loss: 2.6339 - val_acc: 0.7931\n",
      "Epoch 367/400\n",
      "58/58 [==============================] - 0s 406us/step - loss: 2.5333e-06 - acc: 1.0000 - val_loss: 2.6343 - val_acc: 0.7931\n",
      "Epoch 368/400\n",
      "58/58 [==============================] - 0s 411us/step - loss: 2.4856e-06 - acc: 1.0000 - val_loss: 2.6347 - val_acc: 0.7931\n",
      "Epoch 369/400\n",
      "58/58 [==============================] - 0s 419us/step - loss: 2.4390e-06 - acc: 1.0000 - val_loss: 2.6351 - val_acc: 0.7931\n",
      "Epoch 370/400\n",
      "58/58 [==============================] - 0s 409us/step - loss: 2.3861e-06 - acc: 1.0000 - val_loss: 2.6353 - val_acc: 0.7931\n",
      "Epoch 371/400\n",
      "58/58 [==============================] - 0s 414us/step - loss: 2.3468e-06 - acc: 1.0000 - val_loss: 2.6359 - val_acc: 0.7931\n",
      "Epoch 372/400\n",
      "58/58 [==============================] - 0s 413us/step - loss: 2.2991e-06 - acc: 1.0000 - val_loss: 2.6363 - val_acc: 0.7931\n",
      "Epoch 373/400\n",
      "58/58 [==============================] - 0s 668us/step - loss: 2.2545e-06 - acc: 1.0000 - val_loss: 2.6366 - val_acc: 0.7931\n",
      "Epoch 374/400\n",
      "58/58 [==============================] - 0s 417us/step - loss: 2.2110e-06 - acc: 1.0000 - val_loss: 2.6370 - val_acc: 0.7931\n",
      "Epoch 375/400\n",
      "58/58 [==============================] - 0s 413us/step - loss: 2.1716e-06 - acc: 1.0000 - val_loss: 2.6375 - val_acc: 0.7931\n",
      "Epoch 376/400\n",
      "58/58 [==============================] - 0s 416us/step - loss: 2.1260e-06 - acc: 1.0000 - val_loss: 2.6379 - val_acc: 0.7931\n",
      "Epoch 377/400\n",
      "58/58 [==============================] - 0s 416us/step - loss: 2.0845e-06 - acc: 1.0000 - val_loss: 2.6383 - val_acc: 0.7931\n",
      "Epoch 378/400\n",
      "58/58 [==============================] - 0s 419us/step - loss: 2.0420e-06 - acc: 1.0000 - val_loss: 2.6387 - val_acc: 0.7931\n",
      "Epoch 379/400\n",
      "58/58 [==============================] - 0s 410us/step - loss: 2.0047e-06 - acc: 1.0000 - val_loss: 2.6391 - val_acc: 0.7931\n",
      "Epoch 380/400\n",
      "58/58 [==============================] - 0s 405us/step - loss: 1.9684e-06 - acc: 1.0000 - val_loss: 2.6394 - val_acc: 0.7931\n",
      "Epoch 381/400\n",
      "58/58 [==============================] - 0s 415us/step - loss: 1.9342e-06 - acc: 1.0000 - val_loss: 2.6399 - val_acc: 0.7931\n",
      "Epoch 382/400\n",
      "58/58 [==============================] - 0s 411us/step - loss: 1.8917e-06 - acc: 1.0000 - val_loss: 2.6403 - val_acc: 0.7931\n",
      "Epoch 383/400\n",
      "58/58 [==============================] - 0s 400us/step - loss: 1.8544e-06 - acc: 1.0000 - val_loss: 2.6407 - val_acc: 0.7931\n",
      "Epoch 384/400\n",
      "58/58 [==============================] - 0s 398us/step - loss: 1.8192e-06 - acc: 1.0000 - val_loss: 2.6410 - val_acc: 0.7931\n",
      "Epoch 385/400\n",
      "58/58 [==============================] - 0s 407us/step - loss: 1.7881e-06 - acc: 1.0000 - val_loss: 2.6415 - val_acc: 0.7931\n",
      "Epoch 386/400\n",
      "58/58 [==============================] - 0s 410us/step - loss: 1.7611e-06 - acc: 1.0000 - val_loss: 2.6418 - val_acc: 0.7931\n",
      "Epoch 387/400\n",
      "58/58 [==============================] - 0s 406us/step - loss: 1.7290e-06 - acc: 1.0000 - val_loss: 2.6422 - val_acc: 0.7931\n",
      "Epoch 388/400\n",
      "58/58 [==============================] - 0s 412us/step - loss: 1.7021e-06 - acc: 1.0000 - val_loss: 2.6426 - val_acc: 0.7931\n",
      "Epoch 389/400\n",
      "58/58 [==============================] - 0s 420us/step - loss: 1.6730e-06 - acc: 1.0000 - val_loss: 2.6429 - val_acc: 0.7931\n",
      "Epoch 390/400\n",
      "58/58 [==============================] - 0s 405us/step - loss: 1.6440e-06 - acc: 1.0000 - val_loss: 2.6430 - val_acc: 0.7931\n",
      "Epoch 391/400\n",
      "58/58 [==============================] - 0s 406us/step - loss: 1.6181e-06 - acc: 1.0000 - val_loss: 2.6433 - val_acc: 0.7931\n",
      "Epoch 392/400\n",
      "58/58 [==============================] - 0s 406us/step - loss: 1.5891e-06 - acc: 1.0000 - val_loss: 2.6441 - val_acc: 0.7931\n",
      "Epoch 393/400\n",
      "58/58 [==============================] - 0s 542us/step - loss: 1.5621e-06 - acc: 1.0000 - val_loss: 2.6443 - val_acc: 0.7931\n",
      "Epoch 394/400\n",
      "58/58 [==============================] - 0s 407us/step - loss: 1.5341e-06 - acc: 1.0000 - val_loss: 2.6446 - val_acc: 0.7931\n",
      "Epoch 395/400\n",
      "58/58 [==============================] - 0s 413us/step - loss: 1.5103e-06 - acc: 1.0000 - val_loss: 2.6450 - val_acc: 0.7931\n",
      "Epoch 396/400\n",
      "58/58 [==============================] - 0s 424us/step - loss: 1.4865e-06 - acc: 1.0000 - val_loss: 2.6456 - val_acc: 0.7931\n",
      "Epoch 397/400\n",
      "58/58 [==============================] - 0s 443us/step - loss: 1.4554e-06 - acc: 1.0000 - val_loss: 2.6459 - val_acc: 0.7931\n",
      "Epoch 398/400\n",
      "58/58 [==============================] - 0s 401us/step - loss: 1.4305e-06 - acc: 1.0000 - val_loss: 2.6463 - val_acc: 0.7931\n",
      "Epoch 399/400\n",
      "58/58 [==============================] - 0s 400us/step - loss: 1.4046e-06 - acc: 1.0000 - val_loss: 2.6467 - val_acc: 0.7931\n",
      "Epoch 400/400\n",
      "58/58 [==============================] - 0s 417us/step - loss: 1.3838e-06 - acc: 1.0000 - val_loss: 2.6471 - val_acc: 0.7931\n"
     ]
    }
   ],
   "source": [
    "# With epchs 50, the output results where not matching with the expected results\n",
    "# No need to make those changes because accuracy is achieved with the initial values itself\n",
    "history = model.fit(X_train, y_train, batch_size=BATCH_SIZE,\n",
    "                    epochs=NUM_OF_EPOCHS,\n",
    "                    verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88faec43",
   "metadata": {},
   "source": [
    "Testing model for accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb590856",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bb590856",
    "outputId": "af0763cb-91a9-46bc-80df-c03cd4d35dad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 1ms/step - loss: 1.3749 - acc: 0.9167\n",
      "Test Score: 1.3749265670776367\n",
      "Test Accuracy: 0.9166666865348816\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print(\"Test Score:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ed6d84",
   "metadata": {},
   "source": [
    "Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d1e218e8",
   "metadata": {
    "id": "d1e218e8"
   },
   "outputs": [],
   "source": [
    "# Trying out saving the model in h5 file format\n",
    "# Ref: https://www.tensorflow.org/tutorials/keras/save_and_load\n",
    "# We have the model object that needs to be saved\n",
    "# It save text file with Hex numbers in HDF5 format in the current dir\n",
    "# This model file has a size of 39.52 KB\n",
    "model.save('ultasonic_flowClassifyModel.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b58c1da",
   "metadata": {},
   "source": [
    "# Conversion of TensorFlow model to TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "680054a1",
   "metadata": {
    "id": "680054a1",
    "outputId": "d5477c12-d5c2-48d2-b77f-ac2153329c08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object representative_dataset at 0x1191ff430>\n"
     ]
    }
   ],
   "source": [
    "def representative_dataset():\n",
    "    for _ in range(100):\n",
    "      data =  X_test\n",
    "      yield [data.astype(np.float32)]\n",
    "\n",
    "print(representative_dataset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "daf6db3e",
   "metadata": {
    "id": "daf6db3e",
    "outputId": "9b987bb1-86a5-4286-d6c1-b35097b5a86b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_ultasonic_flow_seq_model_keras_dir/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-02 09:10:42.619604: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2024-11-02 09:10:42.835382: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:351] Ignored output_format.\n",
      "2024-11-02 09:10:42.835394: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:354] Ignored drop_control_dependency.\n",
      "2024-11-02 09:10:42.835396: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored change_concat_input_ranges.\n",
      "2024-11-02 09:10:42.835715: I tensorflow/cc/saved_model/reader.cc:38] Reading SavedModel from: saved_ultasonic_flow_seq_model_keras_dir\n",
      "2024-11-02 09:10:42.836370: I tensorflow/cc/saved_model/reader.cc:90] Reading meta graph with tags { serve }\n",
      "2024-11-02 09:10:42.836375: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: saved_ultasonic_flow_seq_model_keras_dir\n",
      "2024-11-02 09:10:42.839352: I tensorflow/cc/saved_model/loader.cc:211] Restoring SavedModel bundle.\n",
      "2024-11-02 09:10:42.860027: I tensorflow/cc/saved_model/loader.cc:195] Running initialization op on SavedModel bundle at path: saved_ultasonic_flow_seq_model_keras_dir\n",
      "2024-11-02 09:10:42.863207: I tensorflow/cc/saved_model/loader.cc:283] SavedModel load for tags { serve }; Status: success: OK. Took 27498 microseconds.\n",
      "2024-11-02 09:10:42.884106: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "INFO: Initialized TensorFlow Lite runtime.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: 0, output_inference_type: 0\n"
     ]
    }
   ],
   "source": [
    "# Converting a tf.Keras model to a TensorFlow Lite model.\n",
    "# It is preferred to use TFLiteConverter from saved model and then\n",
    "# Also provide representative dataset to train the converted TFLite model\n",
    "# Avoid calling the TFLite converter directly from model\n",
    "#converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "tf.saved_model.save(model, \"saved_ultasonic_flow_seq_model_keras_dir\")\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"saved_ultasonic_flow_seq_model_keras_dir\")\n",
    "\n",
    "# Though its size is not much, optimizer is used here to check whether it works on ESP32\n",
    "# if this is chosen, tf.lite.Optimize.OPTIMIZE_FOR_SIZE, the TFLite does not work on ESP32\n",
    "# Observed that even with Optimize.DEFAULT the TFLite model does not work on ESP32\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "22b39332",
   "metadata": {
    "id": "22b39332"
   },
   "outputs": [],
   "source": [
    "# Save the model in TFlite format whose size is just 5 KB\n",
    "# It brings down the size from 49.52 KB to 3.836 KB, 13 times reduction\n",
    "with open('ultasonic_flowClassifyModel.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "464f965d",
   "metadata": {
    "id": "464f965d",
    "outputId": "fe1c06ac-78de-43db-a9f4-1bbcfe1463a1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Initialized TensorFlow Lite runtime.\n"
     ]
    }
   ],
   "source": [
    "# Run the inference on TFLITE model on Python ... here itself first\n",
    "# Let us now first try to run this tflinte model on Python itself\n",
    "# Ref: https://www.tensorflow.org/lite/guide/inference\n",
    "# Load the TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"ultasonic_flowClassifyModel.tflite\")\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d3d58f6d",
   "metadata": {
    "id": "d3d58f6d",
    "outputId": "1825c464-35df-4e47-b1e4-4bb0fcb3a91e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_details:\n",
      " [{'name': 'serving_default_input_1:0', 'index': 0, 'shape': array([ 1, 27], dtype=int32), 'shape_signature': array([-1, 27], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "output_details:\n",
      " [{'name': 'StatefulPartitionedCall:0', 'index': 17, 'shape': array([1, 4], dtype=int32), 'shape_signature': array([-1,  4], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n"
     ]
    }
   ],
   "source": [
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "print('input_details:\\n', input_details)\n",
    "print('output_details:\\n', output_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "013354c5",
   "metadata": {
    "id": "013354c5",
    "outputId": "5d4e89ec-a963-4f5a-a3f5-8ef65d8c28fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.10923219 0.86996994 0.13667912 0.04998717 0.0461822  0.92275136\n",
      "  0.746046   0.63688112 0.2848207  0.3912809  0.02632032 0.155477\n",
      "  0.79883207 0.86733256 0.63005291 0.30329473 0.90649983 0.68553506\n",
      "  0.81327096 0.17307036 0.39214352 0.03327899 0.85221482 0.71211587\n",
      "  0.77083709 0.58047406 0.22967039]]\n",
      "[[0.99609375 0.         0.         0.        ]]\n",
      "[[1.0000000e+00 3.4654166e-10 4.1300779e-16 3.3311268e-10]]\n",
      "[[0.42486196756362915, 0.792097806930542, 0.5861948728561401, 0.5610111951828003, 0.5354955196380615, 0.43262410163879395, 0.5789740085601807, 0.39168402552604675, 0.6487962603569031, 0.4895608425140381, 0.013251636177301407, 0.8533219695091248, 0.7751578092575073, 0.8866511583328247, 0.7434033155441284, 0.7136684060096741, 0.603962779045105, 0.904298722743988, 0.8162965178489685, 0.01068122498691082, 0.01068122498691082, 0.006772009190171957, 0.006772009190171957, 0.002277904422953725, 0.002277904422953725, 0.006507592275738716, 0.006507592275738716]]\n",
      "output1:\n",
      "[[0.99609375 0.         0.         0.        ]]\n",
      "[[1.000000e+00 5.261025e-19 9.730539e-21 1.969884e-15]]\n",
      "[[0.4233745038509369, 0.791832447052002, 0.5857370495796204, 0.9354183077812195, 0.931196928024292, 0.9218912720680237, 0.9492834210395813, 0.39765626192092896, 0.6546631455421448, 0.5017998814582825, 0.01559414528310299, 0.7993514537811279, 0.6766496896743774, 0.7790806889533997, 0.6904633641242981, 0.7447614669799805, 0.6252105236053467, 0.8376504778862, 0.7461320757865906, 0.008544979616999626, 0.008544979616999626, 0.0022573363967239857, 0.0022573363967239857, 0.01138952188193798, 0.01138952188193798, 0.006507592275738716, 0.006507592275738716]]\n",
      "output2:\n",
      "[[0.99609375 0.         0.         0.        ]]\n",
      "[[1.0000000e+00 7.3530145e-21 3.6007755e-18 6.5130343e-16]]\n"
     ]
    }
   ],
   "source": [
    "# Test the model on random input data.\n",
    "input_shape = input_details[0]['shape']\n",
    "#print(input_shape)\n",
    "#print(type(X_test))\n",
    "#print(X_test.iloc[1])\n",
    "#print(X_test.iloc[0])\n",
    "input0_data = np.random.random_sample(input_shape)\n",
    "print(input0_data)\n",
    "input0_data = np.array(input0_data, dtype=np.float32)\n",
    "interpreter.set_tensor(input_details[0]['index'], input0_data)\n",
    "\n",
    "interpreter.invoke()\n",
    "# The function `get_tensor()` returns a copy of the tensor data.\n",
    "# Use `tensor()` in order to get a pointer to the tensor.\n",
    "output0_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(output0_data)\n",
    "\n",
    "# Verify if the same data is given to the original model what is the output\n",
    "output0_data = model.predict(input0_data)\n",
    "print(output0_data)\n",
    "\n",
    "# X_text.iloc[19]: 19th row in ultrasonic flow dataset with output 1.0 0.0 0.0 0.0\n",
    "input1_data = [[0.42486196756362915,\n",
    " 0.792097806930542,\n",
    " 0.5861948728561401,\n",
    " 0.5610111951828003,\n",
    " 0.5354955196380615,\n",
    " 0.43262410163879395,\n",
    " 0.5789740085601807,\n",
    " 0.39168402552604675,\n",
    " 0.6487962603569031,\n",
    " 0.4895608425140381,\n",
    " 0.013251636177301407,\n",
    " 0.8533219695091248,\n",
    " 0.7751578092575073,\n",
    " 0.8866511583328247,\n",
    " 0.7434033155441284,\n",
    " 0.7136684060096741,\n",
    " 0.603962779045105,\n",
    " 0.904298722743988,\n",
    " 0.8162965178489685,\n",
    " 0.01068122498691082,\n",
    " 0.01068122498691082,\n",
    " 0.006772009190171957,\n",
    " 0.006772009190171957,\n",
    " 0.002277904422953725,\n",
    " 0.002277904422953725,\n",
    " 0.006507592275738716,\n",
    " 0.006507592275738716]]\n",
    "print(input1_data)\n",
    "input1_data = np.array(input1_data, dtype=np.float32)\n",
    "interpreter.set_tensor(input_details[0]['index'], input1_data)\n",
    "\n",
    "interpreter.invoke()\n",
    "# The function `get_tensor()` returns a copy of the tensor data.\n",
    "# Use `tensor()` in order to get a pointer to the tensor.\n",
    "output1_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "print('output1:')\n",
    "print(output1_data)\n",
    "\n",
    "# Verify if the same data is given to the original model what is the output\n",
    "output1_data = model.predict(input1_data)\n",
    "print(output1_data)\n",
    "\n",
    "# X_text.iloc[19]: 19th row in ultrasonic flow dataset with output 0.0 0.0 1.0 0.0\n",
    "input2_data = [[0.4233745038509369,\n",
    " 0.791832447052002,\n",
    " 0.5857370495796204,\n",
    " 0.9354183077812195,\n",
    " 0.931196928024292,\n",
    " 0.9218912720680237,\n",
    " 0.9492834210395813,\n",
    " 0.39765626192092896,\n",
    " 0.6546631455421448,\n",
    " 0.5017998814582825,\n",
    " 0.01559414528310299,\n",
    " 0.7993514537811279,\n",
    " 0.6766496896743774,\n",
    " 0.7790806889533997,\n",
    " 0.6904633641242981,\n",
    " 0.7447614669799805,\n",
    " 0.6252105236053467,\n",
    " 0.8376504778862,\n",
    " 0.7461320757865906,\n",
    " 0.008544979616999626,\n",
    " 0.008544979616999626,\n",
    " 0.0022573363967239857,\n",
    " 0.0022573363967239857,\n",
    " 0.01138952188193798,\n",
    " 0.01138952188193798,\n",
    " 0.006507592275738716,\n",
    " 0.006507592275738716]]\n",
    "print(input2_data)\n",
    "input2_data = np.array(input2_data, dtype=np.float32)\n",
    "interpreter.set_tensor(input_details[0]['index'], tf.Variable(input2_data))\n",
    "\n",
    "interpreter.invoke()\n",
    "# The function `get_tensor()` returns a copy of the tensor data.\n",
    "# Use `tensor()` in order to get a pointer to the tensor.\n",
    "output2_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "print('output2:')\n",
    "print(output2_data)\n",
    "\n",
    "# Verify if the same data is given to the original model what is the output\n",
    "output2_data = model.predict(input2_data)\n",
    "print(output2_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e3d9f3c5",
   "metadata": {
    "id": "e3d9f3c5"
   },
   "outputs": [],
   "source": [
    "# Function to convert some hex values into an array for C programming\n",
    "import time, sys\n",
    "\n",
    "# Function to convert some hex values into an array for C programming\n",
    "def hex_to_c_array(hex_data, var_name):\n",
    "    c_str = \"\"\n",
    "\n",
    "    # Create header guard\n",
    "    c_str += '#ifndef ' + var_name.upper() + '_H\\n'\n",
    "    c_str += \"#define \" + var_name.upper() + '_H\\n\\n'\n",
    "\n",
    "    c_str += \"/*\\n Author: Smaran Rangarajan Bharadwaj ft. Mouli Sankaran \\n\"\n",
    "    c_str += \" CAUTION: This is an auto generated file.\\n DO NOT EDIT OR MAKE ANY CHANGES TO IT.\\n\"\n",
    "\n",
    "# Time stamping of this model data in the generated file\n",
    "    localtime = time.asctime( time.localtime(time.time()) )\n",
    "    c_str += \" This model data was generated on \" + localtime+ '\\n\\n'\n",
    "    print(\"This model data was generated on:\", localtime)\n",
    "\n",
    "# Add information about the verisons of tools and packages used in generating this header file\n",
    "    c_str += \" Tools used:\\n Python:\" + str(sys.version) + \"\\n Numpy:\" + str(np.version.version) + \\\n",
    "          \"\\n TensorFlow:\" + str(sys.version) + \"\\n Keras: \"+ str(tf.keras.__version__) + \"\\n\\n\"\n",
    "    print(\"Tools used: Python:\", sys.version, \"\\n Numpy:\", np.version.version, \\\n",
    "          \"\\n TensorFlow:\", sys.version, \"\\n Keras: \", tf.keras.__version__, \"\\n\\n\")\n",
    "\n",
    "# Training details of the model\n",
    "    c_str += ' Model details are:\\n'\n",
    "    c_str += ' NUM_OF_EPOCHS = ' + str(NUM_OF_EPOCHS) + '\\n'\n",
    "    c_str += ' BATCH_SIZE    = ' + str(BATCH_SIZE) + '\\n*/\\n'\n",
    "\n",
    "# Generate 'C' constants for the no. of nodes in each layer\n",
    "    c_str += '\\nconst int ' + 'DENSE1_SIZE' + ' = ' + str(DENSE1_SIZE) + ';\\n'\n",
    "    c_str +=   'const int ' + 'DENSE2_SIZE' + ' = ' + str(DENSE2_SIZE) + ';\\n'\n",
    "    c_str +=   'const int ' + 'DENSE3_SIZE' + ' = ' + str(DENSE3_SIZE) + ';\\n'\n",
    "\n",
    "    # Add array length at the top of the file\n",
    "    c_str += '\\nconst unsigned int ' + var_name + '_len = ' + str(len(hex_data)) + ';\\n'\n",
    "\n",
    "    # Declare C variable\n",
    "    c_str += 'alignas(8) const unsigned char ' + var_name + '[] = {'\n",
    "    hex_array = []\n",
    "    for i, val in enumerate(hex_data):\n",
    "        # Construct string from hex\n",
    "        hex_str = format(val, '#04x')\n",
    "\n",
    "        # Add formating so each line stays within 80 characters\n",
    "        if (i + 1) < len(hex_data):\n",
    "          hex_str += ','\n",
    "        if (i + 1) % 12 == 0:\n",
    "          hex_str += '\\n'\n",
    "        hex_array.append(hex_str)\n",
    "\n",
    "    # Add closing brace\n",
    "    c_str += '\\n' + format(''.join(hex_array)) + '\\n};\\n\\n'\n",
    "\n",
    "    # Close out header guard\n",
    "    c_str += '#endif //' + var_name.upper() + '_H'\n",
    "\n",
    "    return c_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "38be992f",
   "metadata": {
    "id": "38be992f",
    "outputId": "9a0d21d1-2281-4300-911c-c84dc4e02209"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model data was generated on: Sat Nov  2 09:10:45 2024\n",
      "Tools used: Python: 3.8.15 | packaged by conda-forge | (default, Nov 22 2022, 08:52:09) \n",
      "[Clang 14.0.6 ] \n",
      " Numpy: 1.19.5 \n",
      " TensorFlow: 3.8.15 | packaged by conda-forge | (default, Nov 22 2022, 08:52:09) \n",
      "[Clang 14.0.6 ] \n",
      " Keras:  2.6.0 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write TFLite model to a C source (or header) file\n",
    "with open(\"ultasonic_flow_model_esp32\" + '.h', 'w') as file:\n",
    "  file.write(hex_to_c_array(tflite_model, \"ultasonic_flow_model_esp32\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fb6cc704",
   "metadata": {
    "id": "fb6cc704",
    "outputId": "140e549f-8dda-41b1-f21c-45d32d8be9a1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Profile factor</th>\n",
       "      <th>Symmetry</th>\n",
       "      <th>Crossflow</th>\n",
       "      <th>Flow velocity 1</th>\n",
       "      <th>Flow velocity 2</th>\n",
       "      <th>Flow velocity 3</th>\n",
       "      <th>Flow velocity 4</th>\n",
       "      <th>Speed sound 1</th>\n",
       "      <th>Speed sound 2</th>\n",
       "      <th>Speed sound 3</th>\n",
       "      <th>...</th>\n",
       "      <th>Signal quality 4_1</th>\n",
       "      <th>Signal quality 4_2</th>\n",
       "      <th>Gain 1_1</th>\n",
       "      <th>Gain 1_2</th>\n",
       "      <th>Gain 2_1</th>\n",
       "      <th>Gain 2_2</th>\n",
       "      <th>Gain 3_1</th>\n",
       "      <th>Gain 3_2</th>\n",
       "      <th>Gain 4_1</th>\n",
       "      <th>Gain 4_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.044899</td>\n",
       "      <td>0.210775</td>\n",
       "      <td>0.156433</td>\n",
       "      <td>-0.521165</td>\n",
       "      <td>-0.626629</td>\n",
       "      <td>-0.638837</td>\n",
       "      <td>-0.012908</td>\n",
       "      <td>0.044910</td>\n",
       "      <td>0.075289</td>\n",
       "      <td>0.100279</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058326</td>\n",
       "      <td>0.083935</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.989691</td>\n",
       "      <td>-0.979592</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-0.099521</td>\n",
       "      <td>-0.661037</td>\n",
       "      <td>-0.297637</td>\n",
       "      <td>0.093461</td>\n",
       "      <td>-0.007186</td>\n",
       "      <td>-0.006206</td>\n",
       "      <td>0.309482</td>\n",
       "      <td>-0.421626</td>\n",
       "      <td>-0.423557</td>\n",
       "      <td>-0.389044</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017669</td>\n",
       "      <td>-0.028332</td>\n",
       "      <td>0.069007</td>\n",
       "      <td>0.069007</td>\n",
       "      <td>0.057971</td>\n",
       "      <td>0.055172</td>\n",
       "      <td>0.247423</td>\n",
       "      <td>0.244898</td>\n",
       "      <td>-0.006682</td>\n",
       "      <td>-0.006682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>8.013053</td>\n",
       "      <td>49.257046</td>\n",
       "      <td>63.801922</td>\n",
       "      <td>-0.404932</td>\n",
       "      <td>-0.571639</td>\n",
       "      <td>-0.588477</td>\n",
       "      <td>-0.588924</td>\n",
       "      <td>5.500327</td>\n",
       "      <td>2.551963</td>\n",
       "      <td>4.809657</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.083030</td>\n",
       "      <td>-1.149541</td>\n",
       "      <td>6.641907</td>\n",
       "      <td>6.641907</td>\n",
       "      <td>14.202899</td>\n",
       "      <td>13.517241</td>\n",
       "      <td>96.302406</td>\n",
       "      <td>95.319725</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>-0.016054</td>\n",
       "      <td>-0.144443</td>\n",
       "      <td>-0.243299</td>\n",
       "      <td>-0.514493</td>\n",
       "      <td>-0.608507</td>\n",
       "      <td>-0.619640</td>\n",
       "      <td>-0.001646</td>\n",
       "      <td>0.846305</td>\n",
       "      <td>0.927021</td>\n",
       "      <td>0.958217</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049016</td>\n",
       "      <td>0.061380</td>\n",
       "      <td>-0.086259</td>\n",
       "      <td>-0.086259</td>\n",
       "      <td>-0.115942</td>\n",
       "      <td>-0.110345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.332671</td>\n",
       "      <td>33.703640</td>\n",
       "      <td>45.260239</td>\n",
       "      <td>0.678206</td>\n",
       "      <td>0.556910</td>\n",
       "      <td>0.552224</td>\n",
       "      <td>-0.405111</td>\n",
       "      <td>0.555047</td>\n",
       "      <td>0.612933</td>\n",
       "      <td>0.638812</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.070470</td>\n",
       "      <td>-1.114783</td>\n",
       "      <td>-0.051755</td>\n",
       "      <td>-0.051755</td>\n",
       "      <td>-0.173913</td>\n",
       "      <td>-0.165517</td>\n",
       "      <td>-1.237113</td>\n",
       "      <td>-1.224490</td>\n",
       "      <td>1.004454</td>\n",
       "      <td>1.004454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.001474</td>\n",
       "      <td>0.051071</td>\n",
       "      <td>-0.248727</td>\n",
       "      <td>0.483162</td>\n",
       "      <td>0.406311</td>\n",
       "      <td>0.379177</td>\n",
       "      <td>0.487378</td>\n",
       "      <td>-0.382385</td>\n",
       "      <td>-0.381986</td>\n",
       "      <td>-0.351903</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007890</td>\n",
       "      <td>0.004147</td>\n",
       "      <td>-0.017252</td>\n",
       "      <td>-0.017252</td>\n",
       "      <td>-0.115942</td>\n",
       "      <td>-0.110345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.131271</td>\n",
       "      <td>0.081299</td>\n",
       "      <td>-0.628959</td>\n",
       "      <td>-0.514976</td>\n",
       "      <td>-0.617613</td>\n",
       "      <td>-0.640534</td>\n",
       "      <td>-0.001074</td>\n",
       "      <td>0.364944</td>\n",
       "      <td>0.412471</td>\n",
       "      <td>0.448468</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060805</td>\n",
       "      <td>0.080009</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.989691</td>\n",
       "      <td>-0.979592</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.032658</td>\n",
       "      <td>-0.392286</td>\n",
       "      <td>-0.071306</td>\n",
       "      <td>-1.256956</td>\n",
       "      <td>-1.359355</td>\n",
       "      <td>-1.361654</td>\n",
       "      <td>-0.368272</td>\n",
       "      <td>-0.214083</td>\n",
       "      <td>-0.200924</td>\n",
       "      <td>-0.177344</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009038</td>\n",
       "      <td>0.043780</td>\n",
       "      <td>0.099197</td>\n",
       "      <td>0.099197</td>\n",
       "      <td>-0.057971</td>\n",
       "      <td>-0.055172</td>\n",
       "      <td>0.247423</td>\n",
       "      <td>0.244898</td>\n",
       "      <td>-0.004454</td>\n",
       "      <td>-0.004454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0.118195</td>\n",
       "      <td>-0.142810</td>\n",
       "      <td>-0.780520</td>\n",
       "      <td>-0.526387</td>\n",
       "      <td>-0.592885</td>\n",
       "      <td>-0.613479</td>\n",
       "      <td>-0.002505</td>\n",
       "      <td>-0.951820</td>\n",
       "      <td>-0.969515</td>\n",
       "      <td>-0.936862</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001693</td>\n",
       "      <td>0.014451</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115942</td>\n",
       "      <td>0.110345</td>\n",
       "      <td>0.247423</td>\n",
       "      <td>0.244898</td>\n",
       "      <td>-0.004454</td>\n",
       "      <td>-0.004454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.111187</td>\n",
       "      <td>0.117421</td>\n",
       "      <td>0.376623</td>\n",
       "      <td>-0.235030</td>\n",
       "      <td>-0.318827</td>\n",
       "      <td>-0.326227</td>\n",
       "      <td>0.125477</td>\n",
       "      <td>-0.710268</td>\n",
       "      <td>-0.721016</td>\n",
       "      <td>-0.685237</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002524</td>\n",
       "      <td>0.005192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115942</td>\n",
       "      <td>0.110345</td>\n",
       "      <td>0.247423</td>\n",
       "      <td>0.244898</td>\n",
       "      <td>-0.004454</td>\n",
       "      <td>-0.004454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>-0.094319</td>\n",
       "      <td>0.308747</td>\n",
       "      <td>0.047722</td>\n",
       "      <td>-0.484709</td>\n",
       "      <td>-0.594849</td>\n",
       "      <td>-0.611514</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>0.589056</td>\n",
       "      <td>0.648037</td>\n",
       "      <td>0.683380</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047897</td>\n",
       "      <td>0.061254</td>\n",
       "      <td>-0.017252</td>\n",
       "      <td>-0.017252</td>\n",
       "      <td>-0.115942</td>\n",
       "      <td>-0.110345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>22.438614</td>\n",
       "      <td>131.336838</td>\n",
       "      <td>152.169266</td>\n",
       "      <td>0.616415</td>\n",
       "      <td>0.470496</td>\n",
       "      <td>0.369087</td>\n",
       "      <td>-1.104051</td>\n",
       "      <td>7.921953</td>\n",
       "      <td>3.074827</td>\n",
       "      <td>5.270195</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.997499</td>\n",
       "      <td>-1.053371</td>\n",
       "      <td>6.314125</td>\n",
       "      <td>6.314125</td>\n",
       "      <td>14.202899</td>\n",
       "      <td>13.517241</td>\n",
       "      <td>94.762886</td>\n",
       "      <td>93.795921</td>\n",
       "      <td>1.011136</td>\n",
       "      <td>1.011136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>-0.041542</td>\n",
       "      <td>0.351691</td>\n",
       "      <td>-0.247250</td>\n",
       "      <td>-1.255893</td>\n",
       "      <td>-1.361052</td>\n",
       "      <td>-1.370761</td>\n",
       "      <td>-0.370514</td>\n",
       "      <td>-0.049270</td>\n",
       "      <td>-0.019861</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034476</td>\n",
       "      <td>0.066682</td>\n",
       "      <td>0.052234</td>\n",
       "      <td>0.052234</td>\n",
       "      <td>0.231884</td>\n",
       "      <td>0.220690</td>\n",
       "      <td>0.247423</td>\n",
       "      <td>0.244898</td>\n",
       "      <td>-0.004454</td>\n",
       "      <td>-0.004454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>6.713138</td>\n",
       "      <td>35.906498</td>\n",
       "      <td>47.366989</td>\n",
       "      <td>0.485580</td>\n",
       "      <td>0.371764</td>\n",
       "      <td>0.356140</td>\n",
       "      <td>-0.449203</td>\n",
       "      <td>0.233268</td>\n",
       "      <td>0.271132</td>\n",
       "      <td>0.308264</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.040045</td>\n",
       "      <td>-1.125372</td>\n",
       "      <td>-0.051755</td>\n",
       "      <td>-0.051755</td>\n",
       "      <td>-0.173913</td>\n",
       "      <td>-0.165517</td>\n",
       "      <td>-1.237113</td>\n",
       "      <td>-1.224490</td>\n",
       "      <td>1.004454</td>\n",
       "      <td>1.004454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>-0.055914</td>\n",
       "      <td>-0.158737</td>\n",
       "      <td>-1.000219</td>\n",
       "      <td>0.480938</td>\n",
       "      <td>0.415863</td>\n",
       "      <td>0.373195</td>\n",
       "      <td>0.504080</td>\n",
       "      <td>-0.184434</td>\n",
       "      <td>-0.171363</td>\n",
       "      <td>-0.140204</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.041519</td>\n",
       "      <td>-0.050776</td>\n",
       "      <td>0.069007</td>\n",
       "      <td>0.069007</td>\n",
       "      <td>0.057971</td>\n",
       "      <td>0.055172</td>\n",
       "      <td>0.247423</td>\n",
       "      <td>0.244898</td>\n",
       "      <td>-0.006682</td>\n",
       "      <td>-0.006682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.088457</td>\n",
       "      <td>-0.378020</td>\n",
       "      <td>0.397378</td>\n",
       "      <td>0.687296</td>\n",
       "      <td>0.566729</td>\n",
       "      <td>0.579726</td>\n",
       "      <td>0.586538</td>\n",
       "      <td>-0.075431</td>\n",
       "      <td>-0.054965</td>\n",
       "      <td>-0.028784</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008676</td>\n",
       "      <td>-0.011396</td>\n",
       "      <td>-0.017252</td>\n",
       "      <td>-0.017252</td>\n",
       "      <td>-0.115942</td>\n",
       "      <td>-0.110345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.032176</td>\n",
       "      <td>-0.400870</td>\n",
       "      <td>0.027814</td>\n",
       "      <td>-0.743769</td>\n",
       "      <td>-0.839359</td>\n",
       "      <td>-0.840190</td>\n",
       "      <td>-0.114502</td>\n",
       "      <td>-0.367561</td>\n",
       "      <td>-0.401386</td>\n",
       "      <td>-0.402971</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049378</td>\n",
       "      <td>0.070418</td>\n",
       "      <td>0.887025</td>\n",
       "      <td>0.887025</td>\n",
       "      <td>0.863124</td>\n",
       "      <td>0.881226</td>\n",
       "      <td>1.092783</td>\n",
       "      <td>1.122449</td>\n",
       "      <td>0.001856</td>\n",
       "      <td>0.001856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.046720</td>\n",
       "      <td>0.061029</td>\n",
       "      <td>0.606512</td>\n",
       "      <td>0.489544</td>\n",
       "      <td>0.395242</td>\n",
       "      <td>0.394982</td>\n",
       "      <td>0.475735</td>\n",
       "      <td>-0.089383</td>\n",
       "      <td>-0.070670</td>\n",
       "      <td>-0.025070</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035473</td>\n",
       "      <td>-0.039696</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115942</td>\n",
       "      <td>0.110345</td>\n",
       "      <td>0.247423</td>\n",
       "      <td>0.244898</td>\n",
       "      <td>-0.004454</td>\n",
       "      <td>-0.004454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>-0.043563</td>\n",
       "      <td>0.111374</td>\n",
       "      <td>-0.111112</td>\n",
       "      <td>-0.500762</td>\n",
       "      <td>-0.601009</td>\n",
       "      <td>-0.615800</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>-0.553303</td>\n",
       "      <td>-0.561201</td>\n",
       "      <td>-0.532962</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045842</td>\n",
       "      <td>0.058959</td>\n",
       "      <td>-0.017252</td>\n",
       "      <td>-0.017252</td>\n",
       "      <td>-0.115942</td>\n",
       "      <td>-0.110345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>-0.058862</td>\n",
       "      <td>-0.172371</td>\n",
       "      <td>-0.327724</td>\n",
       "      <td>0.135719</td>\n",
       "      <td>0.043787</td>\n",
       "      <td>0.026475</td>\n",
       "      <td>0.321650</td>\n",
       "      <td>0.259429</td>\n",
       "      <td>0.242494</td>\n",
       "      <td>0.254410</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059021</td>\n",
       "      <td>0.062013</td>\n",
       "      <td>0.652690</td>\n",
       "      <td>0.652690</td>\n",
       "      <td>1.579710</td>\n",
       "      <td>1.534100</td>\n",
       "      <td>5.154639</td>\n",
       "      <td>5.136055</td>\n",
       "      <td>0.029819</td>\n",
       "      <td>0.029696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>-0.063626</td>\n",
       "      <td>0.332295</td>\n",
       "      <td>0.322992</td>\n",
       "      <td>-0.484709</td>\n",
       "      <td>-0.595742</td>\n",
       "      <td>-0.608121</td>\n",
       "      <td>0.001837</td>\n",
       "      <td>-0.868106</td>\n",
       "      <td>-0.893764</td>\n",
       "      <td>-0.870938</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031997</td>\n",
       "      <td>0.047230</td>\n",
       "      <td>-0.017252</td>\n",
       "      <td>-0.017252</td>\n",
       "      <td>-0.115942</td>\n",
       "      <td>-0.110345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0.126553</td>\n",
       "      <td>0.628214</td>\n",
       "      <td>-0.033945</td>\n",
       "      <td>0.293630</td>\n",
       "      <td>0.234110</td>\n",
       "      <td>0.193272</td>\n",
       "      <td>0.378818</td>\n",
       "      <td>-0.521910</td>\n",
       "      <td>-0.523326</td>\n",
       "      <td>-0.477252</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031135</td>\n",
       "      <td>-0.038208</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115942</td>\n",
       "      <td>0.110345</td>\n",
       "      <td>0.247423</td>\n",
       "      <td>0.244898</td>\n",
       "      <td>-0.004454</td>\n",
       "      <td>-0.004454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>7.770441</td>\n",
       "      <td>41.835606</td>\n",
       "      <td>54.628159</td>\n",
       "      <td>0.490221</td>\n",
       "      <td>0.373817</td>\n",
       "      <td>0.351675</td>\n",
       "      <td>-0.528608</td>\n",
       "      <td>0.339656</td>\n",
       "      <td>0.387529</td>\n",
       "      <td>0.417827</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.085721</td>\n",
       "      <td>-1.096882</td>\n",
       "      <td>-0.051755</td>\n",
       "      <td>-0.051755</td>\n",
       "      <td>-0.173913</td>\n",
       "      <td>-0.165517</td>\n",
       "      <td>-1.237113</td>\n",
       "      <td>-1.224490</td>\n",
       "      <td>1.004454</td>\n",
       "      <td>1.004454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-0.091937</td>\n",
       "      <td>-0.015542</td>\n",
       "      <td>-0.117329</td>\n",
       "      <td>0.473105</td>\n",
       "      <td>0.368997</td>\n",
       "      <td>0.349086</td>\n",
       "      <td>0.481700</td>\n",
       "      <td>-0.069326</td>\n",
       "      <td>-0.053118</td>\n",
       "      <td>-0.024141</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020722</td>\n",
       "      <td>0.024169</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.989691</td>\n",
       "      <td>-0.979592</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.054113</td>\n",
       "      <td>-0.200084</td>\n",
       "      <td>0.155206</td>\n",
       "      <td>0.317902</td>\n",
       "      <td>0.213935</td>\n",
       "      <td>0.210505</td>\n",
       "      <td>0.404872</td>\n",
       "      <td>0.644866</td>\n",
       "      <td>0.707159</td>\n",
       "      <td>0.731662</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004549</td>\n",
       "      <td>0.009987</td>\n",
       "      <td>-0.017252</td>\n",
       "      <td>-0.017252</td>\n",
       "      <td>-0.115942</td>\n",
       "      <td>-0.110345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>-6.152506</td>\n",
       "      <td>-4.930312</td>\n",
       "      <td>90.712799</td>\n",
       "      <td>-1.782135</td>\n",
       "      <td>-1.803383</td>\n",
       "      <td>-1.846954</td>\n",
       "      <td>-0.611114</td>\n",
       "      <td>2.286026</td>\n",
       "      <td>0.471594</td>\n",
       "      <td>-8.710306</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.968555</td>\n",
       "      <td>-0.977525</td>\n",
       "      <td>7.901282</td>\n",
       "      <td>7.901282</td>\n",
       "      <td>25.507246</td>\n",
       "      <td>24.275862</td>\n",
       "      <td>107.381447</td>\n",
       "      <td>106.285713</td>\n",
       "      <td>1.008909</td>\n",
       "      <td>1.008909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>-0.115474</td>\n",
       "      <td>-0.055130</td>\n",
       "      <td>-0.082437</td>\n",
       "      <td>0.150224</td>\n",
       "      <td>0.039502</td>\n",
       "      <td>0.024600</td>\n",
       "      <td>0.323034</td>\n",
       "      <td>-0.746893</td>\n",
       "      <td>-0.810624</td>\n",
       "      <td>-0.812442</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032209</td>\n",
       "      <td>0.030563</td>\n",
       "      <td>0.755721</td>\n",
       "      <td>0.755721</td>\n",
       "      <td>1.764895</td>\n",
       "      <td>1.753257</td>\n",
       "      <td>5.298969</td>\n",
       "      <td>5.244898</td>\n",
       "      <td>0.028829</td>\n",
       "      <td>0.028706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.038129</td>\n",
       "      <td>0.111704</td>\n",
       "      <td>0.128711</td>\n",
       "      <td>-0.861066</td>\n",
       "      <td>-0.959516</td>\n",
       "      <td>-0.967609</td>\n",
       "      <td>-0.177682</td>\n",
       "      <td>-0.054502</td>\n",
       "      <td>-0.027252</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068014</td>\n",
       "      <td>0.103941</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.989691</td>\n",
       "      <td>-0.979592</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>7.545569</td>\n",
       "      <td>38.097256</td>\n",
       "      <td>49.445320</td>\n",
       "      <td>-0.511205</td>\n",
       "      <td>-0.596456</td>\n",
       "      <td>-0.611782</td>\n",
       "      <td>-0.531375</td>\n",
       "      <td>0.799215</td>\n",
       "      <td>0.874365</td>\n",
       "      <td>0.911792</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.026639</td>\n",
       "      <td>-1.066445</td>\n",
       "      <td>-0.051755</td>\n",
       "      <td>-0.051755</td>\n",
       "      <td>-0.173913</td>\n",
       "      <td>-0.165517</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.004454</td>\n",
       "      <td>1.004454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>-29.165455</td>\n",
       "      <td>-80.823746</td>\n",
       "      <td>-172.681046</td>\n",
       "      <td>-1.782135</td>\n",
       "      <td>-2.068068</td>\n",
       "      <td>-1.198429</td>\n",
       "      <td>-0.612068</td>\n",
       "      <td>-2.899935</td>\n",
       "      <td>12.904850</td>\n",
       "      <td>-7.264624</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.965713</td>\n",
       "      <td>-1.004163</td>\n",
       "      <td>7.901282</td>\n",
       "      <td>7.901282</td>\n",
       "      <td>25.507246</td>\n",
       "      <td>24.275862</td>\n",
       "      <td>107.381447</td>\n",
       "      <td>106.285713</td>\n",
       "      <td>1.008909</td>\n",
       "      <td>1.008909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>-0.146850</td>\n",
       "      <td>-0.429123</td>\n",
       "      <td>-0.011749</td>\n",
       "      <td>0.868029</td>\n",
       "      <td>0.745715</td>\n",
       "      <td>0.747148</td>\n",
       "      <td>0.684983</td>\n",
       "      <td>0.252453</td>\n",
       "      <td>0.289607</td>\n",
       "      <td>0.321263</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029292</td>\n",
       "      <td>-0.027825</td>\n",
       "      <td>-0.017252</td>\n",
       "      <td>-0.017252</td>\n",
       "      <td>-0.115942</td>\n",
       "      <td>-0.110345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.113948</td>\n",
       "      <td>-1.029486</td>\n",
       "      <td>0.453390</td>\n",
       "      <td>-0.752085</td>\n",
       "      <td>-0.844715</td>\n",
       "      <td>-0.827779</td>\n",
       "      <td>-0.115456</td>\n",
       "      <td>-0.815784</td>\n",
       "      <td>-0.002309</td>\n",
       "      <td>-0.006500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049575</td>\n",
       "      <td>0.068534</td>\n",
       "      <td>2.743021</td>\n",
       "      <td>2.743021</td>\n",
       "      <td>0.850242</td>\n",
       "      <td>0.841379</td>\n",
       "      <td>0.742268</td>\n",
       "      <td>0.734694</td>\n",
       "      <td>0.002227</td>\n",
       "      <td>0.002227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>11.191514</td>\n",
       "      <td>56.491493</td>\n",
       "      <td>75.087753</td>\n",
       "      <td>0.459761</td>\n",
       "      <td>0.364979</td>\n",
       "      <td>0.355961</td>\n",
       "      <td>-0.706886</td>\n",
       "      <td>0.056246</td>\n",
       "      <td>0.088222</td>\n",
       "      <td>0.111421</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.075519</td>\n",
       "      <td>-1.039364</td>\n",
       "      <td>-0.051755</td>\n",
       "      <td>-0.051755</td>\n",
       "      <td>-0.173913</td>\n",
       "      <td>-0.165517</td>\n",
       "      <td>-1.237113</td>\n",
       "      <td>-1.224490</td>\n",
       "      <td>1.004454</td>\n",
       "      <td>1.004454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.210000</td>\n",
       "      <td>0.231321</td>\n",
       "      <td>-0.026187</td>\n",
       "      <td>0.461888</td>\n",
       "      <td>0.427468</td>\n",
       "      <td>0.399893</td>\n",
       "      <td>0.469006</td>\n",
       "      <td>-0.004796</td>\n",
       "      <td>0.020785</td>\n",
       "      <td>0.065924</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.041685</td>\n",
       "      <td>-0.053909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115942</td>\n",
       "      <td>0.110345</td>\n",
       "      <td>0.247423</td>\n",
       "      <td>0.244898</td>\n",
       "      <td>-0.004454</td>\n",
       "      <td>-0.004454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.096213</td>\n",
       "      <td>-7.529775</td>\n",
       "      <td>-1.751808</td>\n",
       "      <td>0.576768</td>\n",
       "      <td>0.608150</td>\n",
       "      <td>0.925017</td>\n",
       "      <td>0.780087</td>\n",
       "      <td>-4.957924</td>\n",
       "      <td>-2.440185</td>\n",
       "      <td>-0.685237</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.135137</td>\n",
       "      <td>0.069611</td>\n",
       "      <td>6.305020</td>\n",
       "      <td>6.305020</td>\n",
       "      <td>20.486313</td>\n",
       "      <td>19.595402</td>\n",
       "      <td>77.285225</td>\n",
       "      <td>76.013603</td>\n",
       "      <td>0.577209</td>\n",
       "      <td>0.581106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.111439</td>\n",
       "      <td>-0.180540</td>\n",
       "      <td>-0.436762</td>\n",
       "      <td>0.474459</td>\n",
       "      <td>0.376495</td>\n",
       "      <td>0.353283</td>\n",
       "      <td>0.491530</td>\n",
       "      <td>0.305646</td>\n",
       "      <td>0.342263</td>\n",
       "      <td>0.369545</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012046</td>\n",
       "      <td>0.018819</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.989691</td>\n",
       "      <td>-0.979592</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Profile factor    Symmetry   Crossflow  Flow velocity 1  Flow velocity 2  \\\n",
       "19        -0.044899    0.210775    0.156433        -0.521165        -0.626629   \n",
       "42        -0.099521   -0.661037   -0.297637         0.093461        -0.007186   \n",
       "153        8.013053   49.257046   63.801922        -0.404932        -0.571639   \n",
       "78        -0.016054   -0.144443   -0.243299        -0.514493        -0.608507   \n",
       "145        6.332671   33.703640   45.260239         0.678206         0.556910   \n",
       "15         0.001474    0.051071   -0.248727         0.483162         0.406311   \n",
       "24        -0.131271    0.081299   -0.628959        -0.514976        -0.617613   \n",
       "68         0.032658   -0.392286   -0.071306        -1.256956        -1.359355   \n",
       "113        0.118195   -0.142810   -0.780520        -0.526387        -0.592885   \n",
       "118        0.111187    0.117421    0.376623        -0.235030        -0.318827   \n",
       "93        -0.094319    0.308747    0.047722        -0.484709        -0.594849   \n",
       "159       22.438614  131.336838  152.169266         0.616415         0.470496   \n",
       "69        -0.041542    0.351691   -0.247250        -1.255893        -1.361052   \n",
       "142        6.713138   35.906498   47.366989         0.485580         0.371764   \n",
       "45        -0.055914   -0.158737   -1.000219         0.480938         0.415863   \n",
       "16        -0.088457   -0.378020    0.397378         0.687296         0.566729   \n",
       "51         0.032176   -0.400870    0.027814        -0.743769        -0.839359   \n",
       "125        0.046720    0.061029    0.606512         0.489544         0.395242   \n",
       "96        -0.043563    0.111374   -0.111112        -0.500762        -0.601009   \n",
       "56        -0.058862   -0.172371   -0.327724         0.135719         0.043787   \n",
       "97        -0.063626    0.332295    0.322992        -0.484709        -0.595742   \n",
       "120        0.126553    0.628214   -0.033945         0.293630         0.234110   \n",
       "143        7.770441   41.835606   54.628159         0.490221         0.373817   \n",
       "30        -0.091937   -0.015542   -0.117329         0.473105         0.368997   \n",
       "9         -0.054113   -0.200084    0.155206         0.317902         0.213935   \n",
       "172       -6.152506   -4.930312   90.712799        -1.782135        -1.803383   \n",
       "60        -0.115474   -0.055130   -0.082437         0.150224         0.039502   \n",
       "18         0.038129    0.111704    0.128711        -0.861066        -0.959516   \n",
       "148        7.545569   38.097256   49.445320        -0.511205        -0.596456   \n",
       "173      -29.165455  -80.823746 -172.681046        -1.782135        -2.068068   \n",
       "109       -0.146850   -0.429123   -0.011749         0.868029         0.745715   \n",
       "55         0.113948   -1.029486    0.453390        -0.752085        -0.844715   \n",
       "140       11.191514   56.491493   75.087753         0.459761         0.364979   \n",
       "126        0.210000    0.231321   -0.026187         0.461888         0.427468   \n",
       "66         0.096213   -7.529775   -1.751808         0.576768         0.608150   \n",
       "29        -0.111439   -0.180540   -0.436762         0.474459         0.376495   \n",
       "\n",
       "     Flow velocity 3  Flow velocity 4  Speed sound 1  Speed sound 2  \\\n",
       "19         -0.638837        -0.012908       0.044910       0.075289   \n",
       "42         -0.006206         0.309482      -0.421626      -0.423557   \n",
       "153        -0.588477        -0.588924       5.500327       2.551963   \n",
       "78         -0.619640        -0.001646       0.846305       0.927021   \n",
       "145         0.552224        -0.405111       0.555047       0.612933   \n",
       "15          0.379177         0.487378      -0.382385      -0.381986   \n",
       "24         -0.640534        -0.001074       0.364944       0.412471   \n",
       "68         -1.361654        -0.368272      -0.214083      -0.200924   \n",
       "113        -0.613479        -0.002505      -0.951820      -0.969515   \n",
       "118        -0.326227         0.125477      -0.710268      -0.721016   \n",
       "93         -0.611514         0.004700       0.589056       0.648037   \n",
       "159         0.369087        -1.104051       7.921953       3.074827   \n",
       "69         -1.370761        -0.370514      -0.049270      -0.019861   \n",
       "142         0.356140        -0.449203       0.233268       0.271132   \n",
       "45          0.373195         0.504080      -0.184434      -0.171363   \n",
       "16          0.579726         0.586538      -0.075431      -0.054965   \n",
       "51         -0.840190        -0.114502      -0.367561      -0.401386   \n",
       "125         0.394982         0.475735      -0.089383      -0.070670   \n",
       "96         -0.615800         0.000740      -0.553303      -0.561201   \n",
       "56          0.026475         0.321650       0.259429       0.242494   \n",
       "97         -0.608121         0.001837      -0.868106      -0.893764   \n",
       "120         0.193272         0.378818      -0.521910      -0.523326   \n",
       "143         0.351675        -0.528608       0.339656       0.387529   \n",
       "30          0.349086         0.481700      -0.069326      -0.053118   \n",
       "9           0.210505         0.404872       0.644866       0.707159   \n",
       "172        -1.846954        -0.611114       2.286026       0.471594   \n",
       "60          0.024600         0.323034      -0.746893      -0.810624   \n",
       "18         -0.967609        -0.177682      -0.054502      -0.027252   \n",
       "148        -0.611782        -0.531375       0.799215       0.874365   \n",
       "173        -1.198429        -0.612068      -2.899935      12.904850   \n",
       "109         0.747148         0.684983       0.252453       0.289607   \n",
       "55         -0.827779        -0.115456      -0.815784      -0.002309   \n",
       "140         0.355961        -0.706886       0.056246       0.088222   \n",
       "126         0.399893         0.469006      -0.004796       0.020785   \n",
       "66          0.925017         0.780087      -4.957924      -2.440185   \n",
       "29          0.353283         0.491530       0.305646       0.342263   \n",
       "\n",
       "     Speed sound 3  ...  Signal quality 4_1  Signal quality 4_2  Gain 1_1  \\\n",
       "19        0.100279  ...            0.058326            0.083935  0.000000   \n",
       "42       -0.389044  ...           -0.017669           -0.028332  0.069007   \n",
       "153       4.809657  ...           -1.083030           -1.149541  6.641907   \n",
       "78        0.958217  ...            0.049016            0.061380 -0.086259   \n",
       "145       0.638812  ...           -1.070470           -1.114783 -0.051755   \n",
       "15       -0.351903  ...            0.007890            0.004147 -0.017252   \n",
       "24        0.448468  ...            0.060805            0.080009  0.000000   \n",
       "68       -0.177344  ...            0.009038            0.043780  0.099197   \n",
       "113      -0.936862  ...            0.001693            0.014451  0.000000   \n",
       "118      -0.685237  ...           -0.002524            0.005192  0.000000   \n",
       "93        0.683380  ...            0.047897            0.061254 -0.017252   \n",
       "159       5.270195  ...           -0.997499           -1.053371  6.314125   \n",
       "69        0.000000  ...            0.034476            0.066682  0.052234   \n",
       "142       0.308264  ...           -1.040045           -1.125372 -0.051755   \n",
       "45       -0.140204  ...           -0.041519           -0.050776  0.069007   \n",
       "16       -0.028784  ...           -0.008676           -0.011396 -0.017252   \n",
       "51       -0.402971  ...            0.049378            0.070418  0.887025   \n",
       "125      -0.025070  ...           -0.035473           -0.039696  0.000000   \n",
       "96       -0.532962  ...            0.045842            0.058959 -0.017252   \n",
       "56        0.254410  ...            0.059021            0.062013  0.652690   \n",
       "97       -0.870938  ...            0.031997            0.047230 -0.017252   \n",
       "120      -0.477252  ...           -0.031135           -0.038208  0.000000   \n",
       "143       0.417827  ...           -1.085721           -1.096882 -0.051755   \n",
       "30       -0.024141  ...            0.020722            0.024169  0.000000   \n",
       "9         0.731662  ...            0.004549            0.009987 -0.017252   \n",
       "172      -8.710306  ...           -0.968555           -0.977525  7.901282   \n",
       "60       -0.812442  ...            0.032209            0.030563  0.755721   \n",
       "18        0.000000  ...            0.068014            0.103941  0.000000   \n",
       "148       0.911792  ...           -1.026639           -1.066445 -0.051755   \n",
       "173      -7.264624  ...           -0.965713           -1.004163  7.901282   \n",
       "109       0.321263  ...           -0.029292           -0.027825 -0.017252   \n",
       "55       -0.006500  ...            0.049575            0.068534  2.743021   \n",
       "140       0.111421  ...           -1.075519           -1.039364 -0.051755   \n",
       "126       0.065924  ...           -0.041685           -0.053909  0.000000   \n",
       "66       -0.685237  ...           -0.135137            0.069611  6.305020   \n",
       "29        0.369545  ...            0.012046            0.018819  0.000000   \n",
       "\n",
       "     Gain 1_2   Gain 2_1   Gain 2_2    Gain 3_1    Gain 3_2  Gain 4_1  \\\n",
       "19   0.000000   0.000000   0.000000   -0.989691   -0.979592  0.000000   \n",
       "42   0.069007   0.057971   0.055172    0.247423    0.244898 -0.006682   \n",
       "153  6.641907  14.202899  13.517241   96.302406   95.319725  1.000000   \n",
       "78  -0.086259  -0.115942  -0.110345    0.000000    0.000000  0.000000   \n",
       "145 -0.051755  -0.173913  -0.165517   -1.237113   -1.224490  1.004454   \n",
       "15  -0.017252  -0.115942  -0.110345    0.000000    0.000000  0.000000   \n",
       "24   0.000000   0.000000   0.000000   -0.989691   -0.979592  0.000000   \n",
       "68   0.099197  -0.057971  -0.055172    0.247423    0.244898 -0.004454   \n",
       "113  0.000000   0.115942   0.110345    0.247423    0.244898 -0.004454   \n",
       "118  0.000000   0.115942   0.110345    0.247423    0.244898 -0.004454   \n",
       "93  -0.017252  -0.115942  -0.110345    0.000000    0.000000  0.000000   \n",
       "159  6.314125  14.202899  13.517241   94.762886   93.795921  1.011136   \n",
       "69   0.052234   0.231884   0.220690    0.247423    0.244898 -0.004454   \n",
       "142 -0.051755  -0.173913  -0.165517   -1.237113   -1.224490  1.004454   \n",
       "45   0.069007   0.057971   0.055172    0.247423    0.244898 -0.006682   \n",
       "16  -0.017252  -0.115942  -0.110345    0.000000    0.000000  0.000000   \n",
       "51   0.887025   0.863124   0.881226    1.092783    1.122449  0.001856   \n",
       "125  0.000000   0.115942   0.110345    0.247423    0.244898 -0.004454   \n",
       "96  -0.017252  -0.115942  -0.110345    0.000000    0.000000  0.000000   \n",
       "56   0.652690   1.579710   1.534100    5.154639    5.136055  0.029819   \n",
       "97  -0.017252  -0.115942  -0.110345    0.000000    0.000000  0.000000   \n",
       "120  0.000000   0.115942   0.110345    0.247423    0.244898 -0.004454   \n",
       "143 -0.051755  -0.173913  -0.165517   -1.237113   -1.224490  1.004454   \n",
       "30   0.000000   0.000000   0.000000   -0.989691   -0.979592  0.000000   \n",
       "9   -0.017252  -0.115942  -0.110345    0.000000    0.000000  0.000000   \n",
       "172  7.901282  25.507246  24.275862  107.381447  106.285713  1.008909   \n",
       "60   0.755721   1.764895   1.753257    5.298969    5.244898  0.028829   \n",
       "18   0.000000   0.000000   0.000000   -0.989691   -0.979592  0.000000   \n",
       "148 -0.051755  -0.173913  -0.165517    0.000000    0.000000  1.004454   \n",
       "173  7.901282  25.507246  24.275862  107.381447  106.285713  1.008909   \n",
       "109 -0.017252  -0.115942  -0.110345    0.000000    0.000000  0.000000   \n",
       "55   2.743021   0.850242   0.841379    0.742268    0.734694  0.002227   \n",
       "140 -0.051755  -0.173913  -0.165517   -1.237113   -1.224490  1.004454   \n",
       "126  0.000000   0.115942   0.110345    0.247423    0.244898 -0.004454   \n",
       "66   6.305020  20.486313  19.595402   77.285225   76.013603  0.577209   \n",
       "29   0.000000   0.000000   0.000000   -0.989691   -0.979592  0.000000   \n",
       "\n",
       "     Gain 4_2  \n",
       "19   0.000000  \n",
       "42  -0.006682  \n",
       "153  1.000000  \n",
       "78   0.000000  \n",
       "145  1.004454  \n",
       "15   0.000000  \n",
       "24   0.000000  \n",
       "68  -0.004454  \n",
       "113 -0.004454  \n",
       "118 -0.004454  \n",
       "93   0.000000  \n",
       "159  1.011136  \n",
       "69  -0.004454  \n",
       "142  1.004454  \n",
       "45  -0.006682  \n",
       "16   0.000000  \n",
       "51   0.001856  \n",
       "125 -0.004454  \n",
       "96   0.000000  \n",
       "56   0.029696  \n",
       "97   0.000000  \n",
       "120 -0.004454  \n",
       "143  1.004454  \n",
       "30   0.000000  \n",
       "9    0.000000  \n",
       "172  1.008909  \n",
       "60   0.028706  \n",
       "18   0.000000  \n",
       "148  1.004454  \n",
       "173  1.008909  \n",
       "109  0.000000  \n",
       "55   0.002227  \n",
       "140  1.004454  \n",
       "126 -0.004454  \n",
       "66   0.581106  \n",
       "29   0.000000  \n",
       "\n",
       "[36 rows x 27 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f19ac7b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-6.152505874633789,\n",
       " -4.930311679840088,\n",
       " 90.71279907226562,\n",
       " -1.7821346521377563,\n",
       " -1.8033833503723145,\n",
       " -1.8469539880752563,\n",
       " -0.6111137866973877,\n",
       " 2.2860257625579834,\n",
       " 0.4715935289859772,\n",
       " -8.710306167602539,\n",
       " 0.9712690114974976,\n",
       " -9.214278221130371,\n",
       " -6.4440202713012695,\n",
       " -8.791808128356934,\n",
       " -11.767769813537598,\n",
       " -6.560761451721191,\n",
       " -6.589517116546631,\n",
       " -0.968554675579071,\n",
       " -0.9775245189666748,\n",
       " 7.901281833648682,\n",
       " 7.901281833648682,\n",
       " 25.507246017456055,\n",
       " 24.275861740112305,\n",
       " 107.3814468383789,\n",
       " 106.28571319580078,\n",
       " 1.0089086294174194,\n",
       " 1.0089086294174194]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.loc[172].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "92f28777",
   "metadata": {
    "id": "92f28777",
    "outputId": "d1f550ab-8502-437b-ce9e-f33aa15d637b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class_1    0.0\n",
       "class_2    0.0\n",
       "class_3    0.0\n",
       "class_4    1.0\n",
       "Name: 172, dtype: float32"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.loc[172]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "sem5_py_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
